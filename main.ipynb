{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95efa511",
   "metadata": {},
   "source": [
    "# prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98bac4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0118d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf40127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373f9483",
   "metadata": {},
   "source": [
    "# CMeEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4dad90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5525f83fe949468b53887fe6c23beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/19.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a12348ed29346468d1b3248ba42c2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/647 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4d3d6fd0aa41fe90554d978c4e6082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/107k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47398f3137194d078341df94437795d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/263k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7310717663a94cab947b8b0c4cab2601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9031866c8338421d83d3a7335d02b215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"hfl/chinese-bert-wwm-ext\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24a7b61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ea0f97b23a4552ad4028cd0ba817ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/393M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel \n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fdbb51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('data/model_data/chinese-bert-wwm-ext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "546e4ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/model_data/chinese-bert-wwm-ext/tokenizer_config.json',\n",
       " 'data/model_data/chinese-bert-wwm-ext/special_tokens_map.json',\n",
       " 'data/model_data/chinese-bert-wwm-ext/vocab.txt',\n",
       " 'data/model_data/chinese-bert-wwm-ext/added_tokens.json',\n",
       " 'data/model_data/chinese-bert-wwm-ext/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('data/model_data/chinese-bert-wwm-ext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b22ff21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 14 11:29:37 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   72C    P0    69W /  70W |  14717MiB / 15109MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla T4            Off  | 00000000:00:07.0 Off |                    0 |\n",
      "| N/A   28C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla T4            Off  | 00000000:00:08.0 Off |                    0 |\n",
      "| N/A   72C    P0    73W /  70W |   8282MiB / 15109MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla T4            Off  | 00000000:00:09.0 Off |                    0 |\n",
      "| N/A   30C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e1d676b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running\n",
      "06/14/2022 09:25:16 - INFO - root -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, data_dir='CBLUEDatasets', device=device(type='cuda'), do_predict=False, do_train=True, earlystop_patience=100, epochs=5, eval_batch_size=16, learning_rate=3e-05, logging_steps=200, max_grad_norm=0.0, max_length=128, model_dir='data/model_data', model_name='chinese-bert-wwm-ext', model_type='bert', output_dir='data/output/ee/chinese-bert-wwm-ext', result_output_dir='data/result_output', save_steps=200, seed=2021, task_name='ee', train_batch_size=16, warmup_proportion=0.1, weight_decay=0.01)\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at data/model_data/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "06/14/2022 09:25:26 - INFO - root -   ***** Running training *****\n",
      "06/14/2022 09:25:26 - INFO - root -   Num samples 15000\n",
      "06/14/2022 09:25:26 - INFO - root -   Num epochs 5\n",
      "06/14/2022 09:25:26 - INFO - root -   Num training steps 4690\n",
      "06/14/2022 09:25:26 - INFO - root -   Num warmup steps 469\n",
      "[Training] 200/938 [=====>........................] - ETA: 4:01  loss: 0.5795 \n",
      "06/14/2022 09:26:32 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 09:26:32 - INFO - root -   Num samples 5000\n",
      "06/14/2022 09:27:09 - INFO - root -   ee-chinese-bert-wwm-ext precision: 0.7330736214559621 - recall: 0.7330736214559621 - f1 score: 0.7330736214559622\n",
      "06/14/2022 09:27:09 - INFO - root -   Saving models checkpoint to data/output/ee/chinese-bert-wwm-ext/checkpoint-200\n",
      "[Training] 400/938 [===========>..................] - ETA: 3:51  loss: 0.6415 \n",
      "06/14/2022 09:28:18 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 09:28:18 - INFO - root -   Num samples 5000\n",
      "06/14/2022 09:28:56 - INFO - root -   ee-chinese-bert-wwm-ext precision: 0.7926904574944157 - recall: 0.7926904574944157 - f1 score: 0.7926904574944157\n",
      "06/14/2022 09:28:57 - INFO - root -   Saving models checkpoint to data/output/ee/chinese-bert-wwm-ext/checkpoint-400\n",
      "[Training] 600/938 [==================>...........] - ETA: 2:38  loss: 0.3888 \n",
      "06/14/2022 09:30:08 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 09:30:08 - INFO - root -   Num samples 5000\n",
      "06/14/2022 09:30:48 - INFO - root -   ee-chinese-bert-wwm-ext precision: 0.7926828338580937 - recall: 0.7926828338580937 - f1 score: 0.7926828338580938\n",
      "06/14/2022 09:30:48 - INFO - root -   Earlystopper counter: 1 out of 100\n",
      "[Training] 800/938 [========================>.....] - ETA: 1:07  loss: 0.6784 \n",
      "06/14/2022 09:31:59 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 09:31:59 - INFO - root -   Num samples 5000\n",
      "06/14/2022 09:32:38 - INFO - root -   ee-chinese-bert-wwm-ext precision: 0.797340875650868 - recall: 0.797340875650868 - f1 score: 0.797340875650868\n",
      "06/14/2022 09:32:39 - INFO - root -   Saving models checkpoint to data/output/ee/chinese-bert-wwm-ext/checkpoint-800\n",
      "[Training] 62/938 [>.............................] - ETA: 5:12  loss: 0.3632 1 \n",
      "06/14/2022 09:33:50 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 09:33:50 - INFO - root -   Num samples 5000\n",
      "06/14/2022 09:34:29 - INFO - root -   ee-chinese-bert-wwm-ext precision: 0.796193518384399 - recall: 0.796193518384399 - f1 score: 0.796193518384399\n",
      "06/14/2022 09:34:29 - INFO - root -   Earlystopper counter: 1 out of 100\n",
      "[Training] 262/938 [=======>......................] - ETA: 5:45  loss: 0.4057  \n",
      "06/14/2022 09:35:42 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 09:35:42 - INFO - root -   Num samples 5000\n",
      "06/14/2022 09:36:22 - INFO - root -   ee-chinese-bert-wwm-ext precision: 0.8060356328761693 - recall: 0.8060356328761693 - f1 score: 0.8060356328761693\n",
      "06/14/2022 09:36:22 - INFO - root -   Saving models checkpoint to data/output/ee/chinese-bert-wwm-ext/checkpoint-1200\n",
      "[Training] 462/938 [=============>................] - ETA: 4:13  loss: 0.4425 \n",
      "06/14/2022 09:37:34 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 09:37:34 - INFO - root -   Num samples 5000\n",
      "06/14/2022 09:38:13 - INFO - root -   ee-chinese-bert-wwm-ext precision: 0.8060851865122626 - recall: 0.8060851865122626 - f1 score: 0.8060851865122626\n",
      "06/14/2022 09:38:14 - INFO - root -   Saving models checkpoint to data/output/ee/chinese-bert-wwm-ext/checkpoint-1400\n",
      "[Training] 662/938 [====================>.........] - ETA: 2:29  loss: 0.4778 \n",
      "06/14/2022 09:39:26 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 09:39:26 - INFO - root -   Num samples 5000\n",
      "06/14/2022 09:40:06 - INFO - root -   ee-chinese-bert-wwm-ext precision: 0.8043774919761227 - recall: 0.8043774919761227 - f1 score: 0.8043774919761227\n",
      "06/14/2022 09:40:06 - INFO - root -   Earlystopper counter: 1 out of 100\n",
      "[Training] 862/938 [==========================>...] - ETA: 41s  loss: 1.0221  \n",
      "06/14/2022 09:41:18 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 09:41:18 - INFO - root -   Num samples 5000\n",
      "06/14/2022 09:41:58 - INFO - root -   ee-chinese-bert-wwm-ext precision: 0.801232741993276 - recall: 0.801232741993276 - f1 score: 0.801232741993276\n",
      "06/14/2022 09:41:58 - INFO - root -   Earlystopper counter: 2 out of 100\n",
      "[Training] 124/938 [==>...........................] - ETA: 4:52  loss: 0.3748  \n",
      "06/14/2022 09:43:09 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 09:43:09 - INFO - root -   Num samples 5000\n",
      "06/14/2022 09:43:49 - INFO - root -   ee-chinese-bert-wwm-ext precision: 0.8084103955904888 - recall: 0.8084103955904888 - f1 score: 0.8084103955904888\n",
      "06/14/2022 09:43:49 - INFO - root -   Saving models checkpoint to data/output/ee/chinese-bert-wwm-ext/checkpoint-2000\n",
      "[Training] 324/938 [=========>....................] - ETA: 4:57  loss: 0.3894 \n",
      "06/14/2022 09:45:02 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 09:45:02 - INFO - root -   Num samples 5000\n",
      "06/14/2022 09:45:42 - INFO - root -   ee-chinese-bert-wwm-ext precision: 0.8037523537977145 - recall: 0.8037523537977145 - f1 score: 0.8037523537977145\n",
      "06/14/2022 09:45:42 - INFO - root -   Earlystopper counter: 1 out of 100\n",
      "[Training] 524/938 [===============>..............] - ETA: 3:32  loss: 0.2713 \n",
      "06/14/2022 09:46:54 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 09:46:54 - INFO - root -   Num samples 5000\n",
      "06/14/2022 09:47:33 - INFO - root -   ee-chinese-bert-wwm-ext precision: 0.8092299364951094 - recall: 0.8092299364951094 - f1 score: 0.8092299364951093\n",
      "06/14/2022 09:47:34 - INFO - root -   Saving models checkpoint to data/output/ee/chinese-bert-wwm-ext/checkpoint-2400\n",
      "[Training] 724/938 [======================>.......] - ETA: 1:52  loss: 0.2451 \n",
      "06/14/2022 09:48:46 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 09:48:46 - INFO - root -   Num samples 5000\n",
      "06/14/2022 09:49:25 - INFO - root -   ee-chinese-bert-wwm-ext precision: 0.8104306592158328 - recall: 0.8104306592158328 - f1 score: 0.8104306592158329\n",
      "06/14/2022 09:49:25 - INFO - root -   Saving models checkpoint to data/output/ee/chinese-bert-wwm-ext/checkpoint-2600\n",
      "[Training] 924/938 [============================>.] - ETA: 7s  loss: 0.2944   \n",
      "06/14/2022 09:50:38 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 09:50:38 - INFO - root -   Num samples 5000\n",
      "06/14/2022 09:51:18 - INFO - root -   ee-chinese-bert-wwm-ext precision: 0.809298549222008 - recall: 0.809298549222008 - f1 score: 0.809298549222008\n",
      "06/14/2022 09:51:18 - INFO - root -   Earlystopper counter: 1 out of 100\n",
      "[Training] 186/938 [====>.........................] - ETA: 4:29  loss: 0.3453  \n",
      "06/14/2022 09:52:29 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 09:52:29 - INFO - root -   Num samples 5000\n",
      "06/14/2022 09:53:09 - INFO - root -   ee-chinese-bert-wwm-ext precision: 0.8077623865031143 - recall: 0.8077623865031143 - f1 score: 0.8077623865031143\n",
      "06/14/2022 09:53:09 - INFO - root -   Earlystopper counter: 2 out of 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] 386/938 [===========>..................] - ETA: 4:14  loss: 0.2736 \n",
      "06/14/2022 09:54:21 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 09:54:21 - INFO - root -   Num samples 5000\n",
      "06/14/2022 09:55:00 - INFO - root -   ee-chinese-bert-wwm-ext precision: 0.8063672610561786 - recall: 0.8063672610561786 - f1 score: 0.8063672610561786\n",
      "06/14/2022 09:55:00 - INFO - root -   Earlystopper counter: 3 out of 100\n",
      "[Training] 586/938 [=================>............] - ETA: 2:54  loss: 0.1987 \n",
      "06/14/2022 09:56:13 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 09:56:13 - INFO - root -   Num samples 5000\n",
      "06/14/2022 09:56:53 - INFO - root -   ee-chinese-bert-wwm-ext precision: 0.807156307415511 - recall: 0.807156307415511 - f1 score: 0.807156307415511\n",
      "06/14/2022 09:56:53 - INFO - root -   Earlystopper counter: 4 out of 100\n",
      "[Training] 786/938 [========================>.....] - ETA: 1:17  loss: 0.2431 \n",
      "06/14/2022 09:58:05 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 09:58:05 - INFO - root -   Num samples 5000\n",
      "06/14/2022 09:58:44 - INFO - root -   ee-chinese-bert-wwm-ext precision: 0.806851361962629 - recall: 0.806851361962629 - f1 score: 0.806851361962629\n",
      "06/14/2022 09:58:44 - INFO - root -   Earlystopper counter: 5 out of 100\n",
      "[Training] 48/938 [>.............................] - ETA: 5:19  loss: 0.2487 6 \n",
      "06/14/2022 09:59:56 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 09:59:56 - INFO - root -   Num samples 5000\n",
      "06/14/2022 10:00:36 - INFO - root -   ee-chinese-bert-wwm-ext precision: 0.8073850165051727 - recall: 0.8073850165051727 - f1 score: 0.8073850165051727\n",
      "06/14/2022 10:00:36 - INFO - root -   Earlystopper counter: 6 out of 100\n",
      "[Training] 248/938 [======>.......................] - ETA: 6:01  loss: 0.2541  \n",
      "06/14/2022 10:01:48 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 10:01:48 - INFO - root -   Num samples 5000\n",
      "06/14/2022 10:02:27 - INFO - root -   ee-chinese-bert-wwm-ext precision: 0.8074498174139101 - recall: 0.8074498174139101 - f1 score: 0.8074498174139101\n",
      "06/14/2022 10:02:27 - INFO - root -   Earlystopper counter: 7 out of 100\n",
      "[Training] 448/938 [=============>................] - ETA: 4:23  loss: 0.1858 \n",
      "06/14/2022 10:03:39 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 10:03:39 - INFO - root -   Num samples 5000\n",
      "06/14/2022 10:04:18 - INFO - root -   ee-chinese-bert-wwm-ext precision: 0.8080520846833522 - recall: 0.8080520846833522 - f1 score: 0.8080520846833522\n",
      "06/14/2022 10:04:18 - INFO - root -   Earlystopper counter: 8 out of 100\n",
      "[Training] 648/938 [===================>..........] - ETA: 2:37  loss: 0.3041 \n",
      "06/14/2022 10:05:31 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 10:05:31 - INFO - root -   Num samples 5000\n",
      "06/14/2022 10:06:10 - INFO - root -   ee-chinese-bert-wwm-ext precision: 0.8073011565056301 - recall: 0.8073011565056301 - f1 score: 0.8073011565056301\n",
      "06/14/2022 10:06:10 - INFO - root -   Earlystopper counter: 9 out of 100\n",
      "[Training] 848/938 [==========================>...] - ETA: 49s  loss: 0.2517  \n",
      "06/14/2022 10:07:22 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 10:07:22 - INFO - root -   Num samples 5000\n",
      "06/14/2022 10:08:02 - INFO - root -   ee-chinese-bert-wwm-ext precision: 0.8072249201424095 - recall: 0.8072249201424095 - f1 score: 0.8072249201424095\n",
      "06/14/2022 10:08:02 - INFO - root -   Earlystopper counter: 10 out of 100\n",
      "[Training] 938/938 [==============================] 571.0ms/step  loss: 0.2810 06/14/2022 10:08:34 - INFO - root -   Training Stop! The best step 2600: 0.8104306592158329\n",
      "06/14/2022 10:08:35 - INFO - root -   Saving models checkpoint to data/output/ee/chinese-bert-wwm-ext\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=3  bash examples/run_ee.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "576f2c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running\n",
      "06/14/2022 11:26:57 - INFO - root -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, data_dir='CBLUEDatasets', device=device(type='cuda'), do_predict=True, do_train=False, earlystop_patience=2, epochs=3, eval_batch_size=32, learning_rate=5e-05, logging_steps=10, max_grad_norm=1.0, max_length=128, model_dir='data/model_data', model_name='chinese-bert-wwm-ext', model_type='bert', output_dir='data/output/ee/chinese-bert-wwm-ext', result_output_dir='data/result_output', save_steps=1000, seed=2021, task_name='ee', train_batch_size=8, warmup_proportion=0.1, weight_decay=0.01)\n",
      "06/14/2022 11:27:02 - INFO - root -   ***** Running prediction *****\n",
      "06/14/2022 11:27:02 - INFO - root -   Num samples 3000\n",
      "[Prediction] 94/94 [==============================] 203.4ms/step"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=3  bash examples/run_ee.sh predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cfbbe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
