{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f581cfd9",
   "metadata": {},
   "source": [
    "# prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a9c68f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18aeba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cda34f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d01245d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5525f83fe949468b53887fe6c23beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/19.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a12348ed29346468d1b3248ba42c2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/647 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4d3d6fd0aa41fe90554d978c4e6082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/107k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47398f3137194d078341df94437795d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/263k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7310717663a94cab947b8b0c4cab2601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9031866c8338421d83d3a7335d02b215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"hfl/chinese-bert-wwm-ext\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c72e366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ea0f97b23a4552ad4028cd0ba817ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/393M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel \n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7534af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('data/model_data/chinese-bert-wwm-ext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1d9cc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/model_data/chinese-bert-wwm-ext/tokenizer_config.json',\n",
       " 'data/model_data/chinese-bert-wwm-ext/special_tokens_map.json',\n",
       " 'data/model_data/chinese-bert-wwm-ext/vocab.txt',\n",
       " 'data/model_data/chinese-bert-wwm-ext/added_tokens.json',\n",
       " 'data/model_data/chinese-bert-wwm-ext/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('data/model_data/chinese-bert-wwm-ext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e90cfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 14 20:18:05 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   69C    P0    67W /  70W |  14717MiB / 15109MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla T4            Off  | 00000000:00:07.0 Off |                    0 |\n",
      "| N/A   28C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla T4            Off  | 00000000:00:08.0 Off |                    0 |\n",
      "| N/A   69C    P0    69W /  70W |   8282MiB / 15109MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla T4            Off  | 00000000:00:09.0 Off |                    0 |\n",
      "| N/A   64C    P0    70W /  70W |   5998MiB / 15109MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac9f60",
   "metadata": {},
   "source": [
    "# CHIP-CDN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "456fd9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "724ee750",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running\n",
      "06/15/2022 01:04:49 - INFO - root -   Training CLS model...\n",
      "Building prefix dict from the default dictionary ...\n",
      "06/15/2022 01:04:53 - DEBUG - jieba -   Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "06/15/2022 01:04:53 - DEBUG - jieba -   Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.597 seconds.\n",
      "06/15/2022 01:04:53 - DEBUG - jieba -   Loading model cost 0.597 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "06/15/2022 01:04:53 - DEBUG - jieba -   Prefix dict has been built successfully.\n",
      "06/15/2022 01:04:55 - INFO - gensim.corpora.dictionary -   adding document #0 to Dictionary<0 unique tokens: []>\n",
      "06/15/2022 01:04:55 - INFO - gensim.corpora.dictionary -   adding document #10000 to Dictionary<4399 unique tokens: ['霍乱', ',', '01', '型', '所致']...>\n",
      "06/15/2022 01:04:55 - INFO - gensim.corpora.dictionary -   adding document #20000 to Dictionary<6737 unique tokens: ['霍乱', ',', '01', '型', '所致']...>\n",
      "06/15/2022 01:04:55 - INFO - gensim.corpora.dictionary -   adding document #30000 to Dictionary<8736 unique tokens: ['霍乱', ',', '01', '型', '所致']...>\n",
      "06/15/2022 01:04:55 - INFO - gensim.corpora.dictionary -   built Dictionary<10473 unique tokens: ['霍乱', ',', '01', '型', '所致']...> from 37879 documents (total 204725 corpus positions)\n",
      "06/15/2022 01:04:55 - INFO - gensim.utils -   Dictionary lifecycle event {'msg': \"built Dictionary<10473 unique tokens: ['霍乱', ',', '01', '型', '所致']...> from 37879 documents (total 204725 corpus positions)\", 'datetime': '2022-06-15T01:04:55.620731', 'gensim': '4.2.0', 'python': '3.7.12 (default, Jan 15 2022, 18:48:18) \\n[GCC 7.5.0]', 'platform': 'Linux-3.10.0-1160.42.2.el7.x86_64-x86_64-with-Ubuntu-18.04-bionic', 'event': 'created'}\n",
      "06/15/2022 01:04:55 - INFO - gensim.models.tfidfmodel -   collecting document frequencies\n",
      "06/15/2022 01:04:55 - INFO - gensim.models.tfidfmodel -   PROGRESS: processing document #0\n",
      "06/15/2022 01:04:55 - INFO - gensim.models.tfidfmodel -   PROGRESS: processing document #10000\n",
      "06/15/2022 01:04:55 - INFO - gensim.models.tfidfmodel -   PROGRESS: processing document #20000\n",
      "06/15/2022 01:04:55 - INFO - gensim.models.tfidfmodel -   PROGRESS: processing document #30000\n",
      "06/15/2022 01:04:55 - INFO - gensim.utils -   TfidfModel lifecycle event {'msg': 'calculated IDF weights for 37879 documents and 10473 features (196761 matrix non-zeros)', 'datetime': '2022-06-15T01:04:55.968415', 'gensim': '4.2.0', 'python': '3.7.12 (default, Jan 15 2022, 18:48:18) \\n[GCC 7.5.0]', 'platform': 'Linux-3.10.0-1160.42.2.el7.x86_64-x86_64-with-Ubuntu-18.04-bionic', 'event': 'initialize'}\n",
      "06/15/2022 01:04:55 - INFO - gensim.similarities.docsim -   creating sparse index\n",
      "06/15/2022 01:04:55 - INFO - gensim.matutils -   creating sparse matrix from corpus\n",
      "06/15/2022 01:04:55 - INFO - gensim.matutils -   PROGRESS: at document #0\n",
      "06/15/2022 01:04:56 - INFO - gensim.matutils -   PROGRESS: at document #10000\n",
      "06/15/2022 01:04:56 - INFO - gensim.matutils -   PROGRESS: at document #20000\n",
      "06/15/2022 01:04:56 - INFO - gensim.matutils -   PROGRESS: at document #30000\n",
      "06/15/2022 01:04:57 - INFO - gensim.similarities.docsim -   created <37879x10473 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 196761 stored elements in Compressed Sparse Row format>\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "06/15/2022 01:05:02 - INFO - root -   ***** Running training *****\n",
      "06/15/2022 01:05:02 - INFO - root -   Num samples 125688\n",
      "06/15/2022 01:05:02 - INFO - root -   Num epochs 1\n",
      "06/15/2022 01:05:02 - INFO - root -   Num training steps 3928\n",
      "06/15/2022 01:05:02 - INFO - root -   Num warmup steps 392\n",
      "[Training] 250/3928 [>.............................] - ETA: 20:50  loss: 0.1529 \n",
      "06/15/2022 01:06:27 - INFO - root -   ***** Running evaluation *****\n",
      "06/15/2022 01:06:27 - INFO - root -   Num samples 41252\n",
      "[Evaluation] 162/162 [==============================] 956.4ms/step06/15/2022 01:09:02 - INFO - root -   cdn-chinese-bert-wwm-ext precision: 0.8467292836024201 - recall: 0.8455267927724449 - f1 score: 0.8458540816595312\n",
      "06/15/2022 01:09:02 - INFO - root -   Saving models checkpoint to data/output/cdn/chinese-bert-wwm-ext/checkpoint-250\n",
      "[Training] 500/3928 [==>...........................] - ETA: 37:42  loss: 0.4442 \n",
      "06/15/2022 01:10:32 - INFO - root -   ***** Running evaluation *****\n",
      "06/15/2022 01:10:32 - INFO - root -   Num samples 41252\n",
      "[Evaluation] 162/162 [==============================] 982.9ms/step06/15/2022 01:13:11 - INFO - root -   cdn-chinese-bert-wwm-ext precision: 0.8621761603701357 - recall: 0.8598345002823264 - f1 score: 0.8603347981526401\n",
      "06/15/2022 01:13:11 - INFO - root -   Saving models checkpoint to data/output/cdn/chinese-bert-wwm-ext/checkpoint-500\n",
      "[Training] 750/3928 [====>.........................] - ETA: 40:55  loss: 0.4715 \n",
      "06/15/2022 01:14:41 - INFO - root -   ***** Running evaluation *****\n",
      "06/15/2022 01:14:41 - INFO - root -   Num samples 41252\n",
      "[Evaluation] 162/162 [==============================] 992.4ms/step06/15/2022 01:17:22 - INFO - root -   cdn-chinese-bert-wwm-ext precision: 0.8830920090873842 - recall: 0.8797053500846979 - f1 score: 0.88035567061193\n",
      "06/15/2022 01:17:22 - INFO - root -   Saving models checkpoint to data/output/cdn/chinese-bert-wwm-ext/checkpoint-750\n",
      "[Training] 1000/3928 [======>.......................] - ETA: 40:34  loss: 0.3924 \n",
      "06/15/2022 01:18:53 - INFO - root -   ***** Running evaluation *****\n",
      "06/15/2022 01:18:53 - INFO - root -   Num samples 41252\n",
      "[Evaluation] 162/162 [==============================] 1.0s/step06/15/2022 01:21:35 - INFO - root -   cdn-chinese-bert-wwm-ext precision: 0.8893950234487091 - recall: 0.8788876058723885 - f1 score: 0.8799033923969035\n",
      "06/15/2022 01:21:35 - INFO - root -   Earlystopper counter: 1 out of 100\n",
      "[Training] 1250/3928 [========>.....................] - ETA: 38:40  loss: 0.4691 \n",
      "06/15/2022 01:23:05 - INFO - root -   ***** Running evaluation *****\n",
      "06/15/2022 01:23:05 - INFO - root -   Num samples 41252\n",
      "[Evaluation] 162/162 [==============================] 985.5ms/step06/15/2022 01:25:44 - INFO - root -   cdn-chinese-bert-wwm-ext precision: 0.8926838412169192 - recall: 0.8930336674195369 - f1 score: 0.8925705416067571\n",
      "06/15/2022 01:25:45 - INFO - root -   Saving models checkpoint to data/output/cdn/chinese-bert-wwm-ext/checkpoint-1250\n",
      "[Training] 1500/3928 [==========>...................] - ETA: 35:58  loss: 0.1843 \n",
      "06/15/2022 01:27:15 - INFO - root -   ***** Running evaluation *****\n",
      "06/15/2022 01:27:15 - INFO - root -   Num samples 41252\n",
      "[Evaluation] 162/162 [==============================] 975.3ms/step06/15/2022 01:29:53 - INFO - root -   cdn-chinese-bert-wwm-ext precision: 0.8988039170212829 - recall: 0.8973273291925465 - f1 score: 0.8977456934397929\n",
      "06/15/2022 01:29:54 - INFO - root -   Saving models checkpoint to data/output/cdn/chinese-bert-wwm-ext/checkpoint-1500\n",
      "[Training] 1750/3928 [============>.................] - ETA: 32:48  loss: 0.1556 \n",
      "06/15/2022 01:31:24 - INFO - root -   ***** Running evaluation *****\n",
      "06/15/2022 01:31:24 - INFO - root -   Num samples 41252\n",
      "[Evaluation] 162/162 [==============================] 978.0ms/step06/15/2022 01:34:02 - INFO - root -   cdn-chinese-bert-wwm-ext precision: 0.9038638265430117 - recall: 0.9023094297007341 - f1 score: 0.9027463491783712\n",
      "06/15/2022 01:34:03 - INFO - root -   Saving models checkpoint to data/output/cdn/chinese-bert-wwm-ext/checkpoint-1750\n",
      "[Training] 2000/3928 [==============>...............] - ETA: 29:25  loss: 0.0646 \n",
      "06/15/2022 01:35:33 - INFO - root -   ***** Running evaluation *****\n",
      "06/15/2022 01:35:33 - INFO - root -   Num samples 41252\n",
      "[Evaluation] 162/162 [==============================] 981.4ms/step06/15/2022 01:38:12 - INFO - root -   cdn-chinese-bert-wwm-ext precision: 0.9095438793078168 - recall: 0.9060865471485037 - f1 score: 0.9068040385873939\n",
      "06/15/2022 01:38:12 - INFO - root -   Saving models checkpoint to data/output/cdn/chinese-bert-wwm-ext/checkpoint-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] 2250/3928 [================>.............] - ETA: 25:51  loss: 0.2357 \n",
      "06/15/2022 01:39:42 - INFO - root -   ***** Running evaluation *****\n",
      "06/15/2022 01:39:42 - INFO - root -   Num samples 41252\n",
      "[Evaluation] 162/162 [==============================] 986.3ms/step06/15/2022 01:42:22 - INFO - root -   cdn-chinese-bert-wwm-ext precision: 0.9144177718848038 - recall: 0.9097934217955956 - f1 score: 0.9106439310762653\n",
      "06/15/2022 01:42:22 - INFO - root -   Saving models checkpoint to data/output/cdn/chinese-bert-wwm-ext/checkpoint-2250\n",
      "[Training] 2500/3928 [==================>...........] - ETA: 22:11  loss: 0.1149 \n",
      "06/15/2022 01:43:53 - INFO - root -   ***** Running evaluation *****\n",
      "06/15/2022 01:43:53 - INFO - root -   Num samples 41252\n",
      "[Evaluation] 162/162 [==============================] 1.0s/steps006/15/2022 01:46:35 - INFO - root -   cdn-chinese-bert-wwm-ext precision: 0.916938589699646 - recall: 0.9166521033314512 - f1 score: 0.9167735044573693\n",
      "06/15/2022 01:46:36 - INFO - root -   Saving models checkpoint to data/output/cdn/chinese-bert-wwm-ext/checkpoint-2500\n",
      "[Training] 2750/3928 [====================>.........] - ETA: 18:27  loss: 0.2966 \n",
      "06/15/2022 01:48:06 - INFO - root -   ***** Running evaluation *****\n",
      "06/15/2022 01:48:06 - INFO - root -   Num samples 41252\n",
      "[Evaluation] 162/162 [==============================] 988.9ms/step06/15/2022 01:50:46 - INFO - root -   cdn-chinese-bert-wwm-ext precision: 0.918153933204081 - recall: 0.9167544466403162 - f1 score: 0.9171721123452663\n",
      "06/15/2022 01:50:47 - INFO - root -   Saving models checkpoint to data/output/cdn/chinese-bert-wwm-ext/checkpoint-2750\n",
      "[Training] 3000/3928 [=====================>........] - ETA: 14:37  loss: 0.2843 \n",
      "06/15/2022 01:52:17 - INFO - root -   ***** Running evaluation *****\n",
      "06/15/2022 01:52:17 - INFO - root -   Num samples 41252\n",
      "[Evaluation] 162/162 [==============================] 986.8ms/step06/15/2022 01:54:57 - INFO - root -   cdn-chinese-bert-wwm-ext precision: 0.9158537272871792 - recall: 0.9161689440993789 - f1 score: 0.9159407043537557\n",
      "06/15/2022 01:54:57 - INFO - root -   Earlystopper counter: 1 out of 100\n",
      "[Training] 3250/3928 [=======================>......] - ETA: 10:43  loss: 0.2180 \n",
      "06/15/2022 01:56:26 - INFO - root -   ***** Running evaluation *****\n",
      "06/15/2022 01:56:26 - INFO - root -   Num samples 41252\n",
      "[Evaluation] 162/162 [==============================] 980.8ms/step06/15/2022 01:59:05 - INFO - root -   cdn-chinese-bert-wwm-ext precision: 0.9185098191479876 - recall: 0.9187333709768493 - f1 score: 0.918595421708839\n",
      "06/15/2022 01:59:06 - INFO - root -   Saving models checkpoint to data/output/cdn/chinese-bert-wwm-ext/checkpoint-3250\n",
      "[Training] 3500/3928 [=========================>....] - ETA: 6:47  loss: 0.0262  \n",
      "06/15/2022 02:00:36 - INFO - root -   ***** Running evaluation *****\n",
      "06/15/2022 02:00:36 - INFO - root -   Num samples 41252\n",
      "[Evaluation] 162/162 [==============================] 993.8ms/step06/15/2022 02:03:17 - INFO - root -   cdn-chinese-bert-wwm-ext precision: 0.9216924647804987 - recall: 0.9219995341614906 - f1 score: 0.9217836590145758\n",
      "06/15/2022 02:03:17 - INFO - root -   Saving models checkpoint to data/output/cdn/chinese-bert-wwm-ext/checkpoint-3500\n",
      "[Training] 3750/3928 [===========================>..] - ETA: 2:50  loss: 0.2635 \n",
      "06/15/2022 02:04:48 - INFO - root -   ***** Running evaluation *****\n",
      "06/15/2022 02:04:48 - INFO - root -   Num samples 41252\n",
      "[Evaluation] 162/162 [==============================] 987.6ms/step06/15/2022 02:07:28 - INFO - root -   cdn-chinese-bert-wwm-ext precision: 0.9211105621429578 - recall: 0.9214172077922078 - f1 score: 0.9212015839188703\n",
      "06/15/2022 02:07:28 - INFO - root -   Earlystopper counter: 1 out of 100\n",
      "[Training] 3928/3928 [==============================] 969.9ms/step  loss: 0.2340 06/15/2022 02:08:31 - INFO - root -   Training Stop! The best step 3500: 0.9217836590145758\n",
      "06/15/2022 02:08:33 - INFO - root -   Saving models checkpoint to data/output/cdn/chinese-bert-wwm-ext/cls\n",
      "06/15/2022 02:08:33 - INFO - root -   Training NUM model...\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "06/15/2022 02:08:34 - INFO - root -   ***** Running training *****\n",
      "06/15/2022 02:08:34 - INFO - root -   Num samples 6000\n",
      "06/15/2022 02:08:34 - INFO - root -   Num epochs 1\n",
      "06/15/2022 02:08:34 - INFO - root -   Num training steps 188\n",
      "06/15/2022 02:08:34 - INFO - root -   Num warmup steps 18\n",
      "[Training] 30/188 [===>..........................] - ETA: 53s  loss: 0.9511  \n",
      "06/15/2022 02:08:44 - INFO - root -   ***** Running evaluation *****\n",
      "06/15/2022 02:08:44 - INFO - root -   Num samples 2000\n",
      "06/15/2022 02:08:52 - INFO - root -   cdn-chinese-bert-wwm-ext f1: 0.6043114588408779\n",
      "06/15/2022 02:08:52 - INFO - root -   Saving models checkpoint to data/output/cdn/chinese-bert-wwm-ext/checkpoint-30\n",
      "[Training] 60/188 [========>.....................] - ETA: 1:00  loss: 0.8077 \n",
      "06/15/2022 02:09:03 - INFO - root -   ***** Running evaluation *****\n",
      "06/15/2022 02:09:03 - INFO - root -   Num samples 2000\n",
      "06/15/2022 02:09:10 - INFO - root -   cdn-chinese-bert-wwm-ext f1: 0.6282784987807452\n",
      "06/15/2022 02:09:10 - INFO - root -   Saving models checkpoint to data/output/cdn/chinese-bert-wwm-ext/checkpoint-60\n",
      "[Training] 90/188 [=============>................] - ETA: 50s  loss: 0.7303  \n",
      "06/15/2022 02:09:21 - INFO - root -   ***** Running evaluation *****\n",
      "06/15/2022 02:09:21 - INFO - root -   Num samples 2000\n",
      "06/15/2022 02:09:28 - INFO - root -   cdn-chinese-bert-wwm-ext f1: 0.6717265240220934\n",
      "06/15/2022 02:09:29 - INFO - root -   Saving models checkpoint to data/output/cdn/chinese-bert-wwm-ext/checkpoint-90\n",
      "[Training] 120/188 [==================>...........] - ETA: 36s  loss: 0.7678 \n",
      "06/15/2022 02:09:39 - INFO - root -   ***** Running evaluation *****\n",
      "06/15/2022 02:09:39 - INFO - root -   Num samples 2000\n",
      "06/15/2022 02:09:46 - INFO - root -   cdn-chinese-bert-wwm-ext f1: 0.6784198189750841\n",
      "06/15/2022 02:09:47 - INFO - root -   Saving models checkpoint to data/output/cdn/chinese-bert-wwm-ext/checkpoint-120\n",
      "[Training] 150/188 [======================>.......] - ETA: 21s  loss: 0.7200 \n",
      "06/15/2022 02:09:57 - INFO - root -   ***** Running evaluation *****\n",
      "06/15/2022 02:09:57 - INFO - root -   Num samples 2000\n",
      "06/15/2022 02:10:05 - INFO - root -   cdn-chinese-bert-wwm-ext f1: 0.7013996181715559\n",
      "06/15/2022 02:10:05 - INFO - root -   Saving models checkpoint to data/output/cdn/chinese-bert-wwm-ext/checkpoint-150\n",
      "[Training] 180/188 [===========================>..] - ETA: 4s  loss: 0.7641  \n",
      "06/15/2022 02:10:15 - INFO - root -   ***** Running evaluation *****\n",
      "06/15/2022 02:10:15 - INFO - root -   Num samples 2000\n",
      "06/15/2022 02:10:23 - INFO - root -   cdn-chinese-bert-wwm-ext f1: 0.7151080470006126\n",
      "06/15/2022 02:10:24 - INFO - root -   Saving models checkpoint to data/output/cdn/chinese-bert-wwm-ext/checkpoint-180\n",
      "[Training] 188/188 [==============================] 595.4ms/step  loss: 0.4727 06/15/2022 02:10:26 - INFO - root -   Training Stop! The best step 180: 0.7151080470006126\n",
      "06/15/2022 02:10:27 - INFO - root -   Saving models checkpoint to data/output/cdn/chinese-bert-wwm-ext/num\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1  bash examples/run_cdn.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010b2cc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running\n",
      "Building prefix dict from the default dictionary ...\n",
      "06/15/2022 02:10:34 - DEBUG - jieba -   Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "06/15/2022 02:10:34 - DEBUG - jieba -   Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.588 seconds.\n",
      "06/15/2022 02:10:35 - DEBUG - jieba -   Loading model cost 0.588 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "06/15/2022 02:10:35 - DEBUG - jieba -   Prefix dict has been built successfully.\n",
      "06/15/2022 02:10:36 - INFO - gensim.corpora.dictionary -   adding document #0 to Dictionary<0 unique tokens: []>\n",
      "06/15/2022 02:10:36 - INFO - gensim.corpora.dictionary -   adding document #10000 to Dictionary<4399 unique tokens: ['霍乱', ',', '01', '型', '所致']...>\n",
      "06/15/2022 02:10:36 - INFO - gensim.corpora.dictionary -   adding document #20000 to Dictionary<6737 unique tokens: ['霍乱', ',', '01', '型', '所致']...>\n",
      "06/15/2022 02:10:36 - INFO - gensim.corpora.dictionary -   adding document #30000 to Dictionary<8736 unique tokens: ['霍乱', ',', '01', '型', '所致']...>\n",
      "06/15/2022 02:10:36 - INFO - gensim.corpora.dictionary -   built Dictionary<10473 unique tokens: ['霍乱', ',', '01', '型', '所致']...> from 37879 documents (total 204725 corpus positions)\n",
      "06/15/2022 02:10:36 - INFO - gensim.utils -   Dictionary lifecycle event {'msg': \"built Dictionary<10473 unique tokens: ['霍乱', ',', '01', '型', '所致']...> from 37879 documents (total 204725 corpus positions)\", 'datetime': '2022-06-15T02:10:36.925302', 'gensim': '4.2.0', 'python': '3.7.12 (default, Jan 15 2022, 18:48:18) \\n[GCC 7.5.0]', 'platform': 'Linux-3.10.0-1160.42.2.el7.x86_64-x86_64-with-Ubuntu-18.04-bionic', 'event': 'created'}\n",
      "06/15/2022 02:10:37 - INFO - gensim.models.tfidfmodel -   collecting document frequencies\n",
      "06/15/2022 02:10:37 - INFO - gensim.models.tfidfmodel -   PROGRESS: processing document #0\n",
      "06/15/2022 02:10:37 - INFO - gensim.models.tfidfmodel -   PROGRESS: processing document #10000\n",
      "06/15/2022 02:10:37 - INFO - gensim.models.tfidfmodel -   PROGRESS: processing document #20000\n",
      "06/15/2022 02:10:37 - INFO - gensim.models.tfidfmodel -   PROGRESS: processing document #30000\n",
      "06/15/2022 02:10:37 - INFO - gensim.utils -   TfidfModel lifecycle event {'msg': 'calculated IDF weights for 37879 documents and 10473 features (196761 matrix non-zeros)', 'datetime': '2022-06-15T02:10:37.275142', 'gensim': '4.2.0', 'python': '3.7.12 (default, Jan 15 2022, 18:48:18) \\n[GCC 7.5.0]', 'platform': 'Linux-3.10.0-1160.42.2.el7.x86_64-x86_64-with-Ubuntu-18.04-bionic', 'event': 'initialize'}\n",
      "06/15/2022 02:10:37 - INFO - gensim.similarities.docsim -   creating sparse index\n",
      "06/15/2022 02:10:37 - INFO - gensim.matutils -   creating sparse matrix from corpus\n",
      "06/15/2022 02:10:37 - INFO - gensim.matutils -   PROGRESS: at document #0\n",
      "06/15/2022 02:10:37 - INFO - gensim.matutils -   PROGRESS: at document #10000\n",
      "06/15/2022 02:10:37 - INFO - gensim.matutils -   PROGRESS: at document #20000\n",
      "06/15/2022 02:10:38 - INFO - gensim.matutils -   PROGRESS: at document #30000\n",
      "06/15/2022 02:10:38 - INFO - gensim.similarities.docsim -   created <37879x10473 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 196761 stored elements in Compressed Sparse Row format>\n",
      "100%|█████████████████████████████████████| 10000/10000 [11:11<00:00, 14.90it/s]\n",
      "06/15/2022 02:22:01 - INFO - root -   ***** Running prediction *****\n",
      "06/15/2022 02:22:01 - INFO - root -   Num samples 2000000\n",
      "[Evaluation] 212/7813 [..............................] - ETA: 3:57:23"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=3  bash examples/run_cdn.sh predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6c33a",
   "metadata": {},
   "source": [
    "# CMeIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca3f78e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running\n",
      "06/14/2022 20:13:01 - INFO - root -   Training ER model...\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "06/14/2022 20:13:05 - INFO - root -   ***** Running training *****\n",
      "06/14/2022 20:13:05 - INFO - root -   Num samples 14339\n",
      "06/14/2022 20:13:05 - INFO - root -   Num epochs 1\n",
      "06/14/2022 20:13:05 - INFO - root -   Num training steps 449\n",
      "06/14/2022 20:13:05 - INFO - root -   Num warmup steps 44\n",
      "[Training] 200/449 [============>.................] - ETA: 2:44  loss: 0.0909 \n",
      "06/14/2022 20:15:18 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 20:15:18 - INFO - root -   Num samples 3585\n",
      "06/14/2022 20:16:05 - INFO - root -   ie-checkpoint-3000 f1 score: 0.8931395268041857\n",
      "06/14/2022 20:16:06 - INFO - root -   Saving models checkpoint to data/output/ie/checkpoint-3000/checkpoint-200\n",
      "[Training] 253/449 [===============>..............] - ETA: 2:48  loss: 0.0751 "
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=3  bash examples/run_ie.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb68160d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running\n",
      "06/14/2022 19:55:40 - INFO - root -   Training ER model...\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "06/14/2022 19:55:45 - INFO - root -   ***** Running training *****\n",
      "06/14/2022 19:55:45 - INFO - root -   Num samples 14339\n",
      "06/14/2022 19:55:45 - INFO - root -   Num epochs 1\n",
      "06/14/2022 19:55:45 - INFO - root -   Num training steps 449\n",
      "06/14/2022 19:55:45 - INFO - root -   Num warmup steps 44\n",
      "[Training] 200/449 [============>.................] - ETA: 2:45  loss: 0.2465 \n",
      "06/14/2022 19:57:58 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 19:57:58 - INFO - root -   Num samples 3585\n",
      "06/14/2022 19:58:45 - INFO - root -   ie-chinese-bert-wwm-ext f1 score: 0.8246199385390132\n",
      "06/14/2022 19:58:46 - INFO - root -   Saving models checkpoint to data/output/ie/chinese-bert-wwm-ext/checkpoint-200\n",
      "[Training] 400/449 [=========================>....] - ETA: 39s  loss: 0.1753  \n",
      "06/14/2022 20:01:06 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 20:01:06 - INFO - root -   Num samples 3585\n",
      "06/14/2022 20:01:54 - INFO - root -   ie-chinese-bert-wwm-ext f1 score: 0.8537684861784975\n",
      "06/14/2022 20:01:55 - INFO - root -   Saving models checkpoint to data/output/ie/chinese-bert-wwm-ext/checkpoint-400\n",
      "[Training] 449/449 [==============================] 898.2ms/step  loss: 0.1518 06/14/2022 20:02:28 - INFO - root -   Training Stop! The best step 400: 0.8537684861784975\n",
      "Traceback (most recent call last):\n",
      "  File \"baselines/run_ie.py\", line 220, in <module>\n",
      "    main()\n",
      "  File \"baselines/run_ie.py\", line 123, in main\n",
      "    model.load_state_dict(torch.load(os.path.join(args.output_dir, f'checkpoint-{best_step}', 'pytorch_model.bin')))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1498, in load_state_dict\n",
      "    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
      "RuntimeError: Error(s) in loading state_dict for ERModel:\n",
      "\tMissing key(s) in state_dict: \"encoder.embeddings.position_ids\", \"encoder.embeddings.word_embeddings.weight\", \"encoder.embeddings.position_embeddings.weight\", \"encoder.embeddings.token_type_embeddings.weight\", \"encoder.embeddings.LayerNorm.weight\", \"encoder.embeddings.LayerNorm.bias\", \"encoder.encoder.layer.0.attention.self.query.weight\", \"encoder.encoder.layer.0.attention.self.query.bias\", \"encoder.encoder.layer.0.attention.self.key.weight\", \"encoder.encoder.layer.0.attention.self.key.bias\", \"encoder.encoder.layer.0.attention.self.value.weight\", \"encoder.encoder.layer.0.attention.self.value.bias\", \"encoder.encoder.layer.0.attention.output.dense.weight\", \"encoder.encoder.layer.0.attention.output.dense.bias\", \"encoder.encoder.layer.0.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.0.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.0.intermediate.dense.weight\", \"encoder.encoder.layer.0.intermediate.dense.bias\", \"encoder.encoder.layer.0.output.dense.weight\", \"encoder.encoder.layer.0.output.dense.bias\", \"encoder.encoder.layer.0.output.LayerNorm.weight\", \"encoder.encoder.layer.0.output.LayerNorm.bias\", \"encoder.encoder.layer.1.attention.self.query.weight\", \"encoder.encoder.layer.1.attention.self.query.bias\", \"encoder.encoder.layer.1.attention.self.key.weight\", \"encoder.encoder.layer.1.attention.self.key.bias\", \"encoder.encoder.layer.1.attention.self.value.weight\", \"encoder.encoder.layer.1.attention.self.value.bias\", \"encoder.encoder.layer.1.attention.output.dense.weight\", \"encoder.encoder.layer.1.attention.output.dense.bias\", \"encoder.encoder.layer.1.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.1.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.1.intermediate.dense.weight\", \"encoder.encoder.layer.1.intermediate.dense.bias\", \"encoder.encoder.layer.1.output.dense.weight\", \"encoder.encoder.layer.1.output.dense.bias\", \"encoder.encoder.layer.1.output.LayerNorm.weight\", \"encoder.encoder.layer.1.output.LayerNorm.bias\", \"encoder.encoder.layer.2.attention.self.query.weight\", \"encoder.encoder.layer.2.attention.self.query.bias\", \"encoder.encoder.layer.2.attention.self.key.weight\", \"encoder.encoder.layer.2.attention.self.key.bias\", \"encoder.encoder.layer.2.attention.self.value.weight\", \"encoder.encoder.layer.2.attention.self.value.bias\", \"encoder.encoder.layer.2.attention.output.dense.weight\", \"encoder.encoder.layer.2.attention.output.dense.bias\", \"encoder.encoder.layer.2.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.2.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.2.intermediate.dense.weight\", \"encoder.encoder.layer.2.intermediate.dense.bias\", \"encoder.encoder.layer.2.output.dense.weight\", \"encoder.encoder.layer.2.output.dense.bias\", \"encoder.encoder.layer.2.output.LayerNorm.weight\", \"encoder.encoder.layer.2.output.LayerNorm.bias\", \"encoder.encoder.layer.3.attention.self.query.weight\", \"encoder.encoder.layer.3.attention.self.query.bias\", \"encoder.encoder.layer.3.attention.self.key.weight\", \"encoder.encoder.layer.3.attention.self.key.bias\", \"encoder.encoder.layer.3.attention.self.value.weight\", \"encoder.encoder.layer.3.attention.self.value.bias\", \"encoder.encoder.layer.3.attention.output.dense.weight\", \"encoder.encoder.layer.3.attention.output.dense.bias\", \"encoder.encoder.layer.3.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.3.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.3.intermediate.dense.weight\", \"encoder.encoder.layer.3.intermediate.dense.bias\", \"encoder.encoder.layer.3.output.dense.weight\", \"encoder.encoder.layer.3.output.dense.bias\", \"encoder.encoder.layer.3.output.LayerNorm.weight\", \"encoder.encoder.layer.3.output.LayerNorm.bias\", \"encoder.encoder.layer.4.attention.self.query.weight\", \"encoder.encoder.layer.4.attention.self.query.bias\", \"encoder.encoder.layer.4.attention.self.key.weight\", \"encoder.encoder.layer.4.attention.self.key.bias\", \"encoder.encoder.layer.4.attention.self.value.weight\", \"encoder.encoder.layer.4.attention.self.value.bias\", \"encoder.encoder.layer.4.attention.output.dense.weight\", \"encoder.encoder.layer.4.attention.output.dense.bias\", \"encoder.encoder.layer.4.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.4.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.4.intermediate.dense.weight\", \"encoder.encoder.layer.4.intermediate.dense.bias\", \"encoder.encoder.layer.4.output.dense.weight\", \"encoder.encoder.layer.4.output.dense.bias\", \"encoder.encoder.layer.4.output.LayerNorm.weight\", \"encoder.encoder.layer.4.output.LayerNorm.bias\", \"encoder.encoder.layer.5.attention.self.query.weight\", \"encoder.encoder.layer.5.attention.self.query.bias\", \"encoder.encoder.layer.5.attention.self.key.weight\", \"encoder.encoder.layer.5.attention.self.key.bias\", \"encoder.encoder.layer.5.attention.self.value.weight\", \"encoder.encoder.layer.5.attention.self.value.bias\", \"encoder.encoder.layer.5.attention.output.dense.weight\", \"encoder.encoder.layer.5.attention.output.dense.bias\", \"encoder.encoder.layer.5.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.5.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.5.intermediate.dense.weight\", \"encoder.encoder.layer.5.intermediate.dense.bias\", \"encoder.encoder.layer.5.output.dense.weight\", \"encoder.encoder.layer.5.output.dense.bias\", \"encoder.encoder.layer.5.output.LayerNorm.weight\", \"encoder.encoder.layer.5.output.LayerNorm.bias\", \"encoder.encoder.layer.6.attention.self.query.weight\", \"encoder.encoder.layer.6.attention.self.query.bias\", \"encoder.encoder.layer.6.attention.self.key.weight\", \"encoder.encoder.layer.6.attention.self.key.bias\", \"encoder.encoder.layer.6.attention.self.value.weight\", \"encoder.encoder.layer.6.attention.self.value.bias\", \"encoder.encoder.layer.6.attention.output.dense.weight\", \"encoder.encoder.layer.6.attention.output.dense.bias\", \"encoder.encoder.layer.6.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.6.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.6.intermediate.dense.weight\", \"encoder.encoder.layer.6.intermediate.dense.bias\", \"encoder.encoder.layer.6.output.dense.weight\", \"encoder.encoder.layer.6.output.dense.bias\", \"encoder.encoder.layer.6.output.LayerNorm.weight\", \"encoder.encoder.layer.6.output.LayerNorm.bias\", \"encoder.encoder.layer.7.attention.self.query.weight\", \"encoder.encoder.layer.7.attention.self.query.bias\", \"encoder.encoder.layer.7.attention.self.key.weight\", \"encoder.encoder.layer.7.attention.self.key.bias\", \"encoder.encoder.layer.7.attention.self.value.weight\", \"encoder.encoder.layer.7.attention.self.value.bias\", \"encoder.encoder.layer.7.attention.output.dense.weight\", \"encoder.encoder.layer.7.attention.output.dense.bias\", \"encoder.encoder.layer.7.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.7.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.7.intermediate.dense.weight\", \"encoder.encoder.layer.7.intermediate.dense.bias\", \"encoder.encoder.layer.7.output.dense.weight\", \"encoder.encoder.layer.7.output.dense.bias\", \"encoder.encoder.layer.7.output.LayerNorm.weight\", \"encoder.encoder.layer.7.output.LayerNorm.bias\", \"encoder.encoder.layer.8.attention.self.query.weight\", \"encoder.encoder.layer.8.attention.self.query.bias\", \"encoder.encoder.layer.8.attention.self.key.weight\", \"encoder.encoder.layer.8.attention.self.key.bias\", \"encoder.encoder.layer.8.attention.self.value.weight\", \"encoder.encoder.layer.8.attention.self.value.bias\", \"encoder.encoder.layer.8.attention.output.dense.weight\", \"encoder.encoder.layer.8.attention.output.dense.bias\", \"encoder.encoder.layer.8.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.8.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.8.intermediate.dense.weight\", \"encoder.encoder.layer.8.intermediate.dense.bias\", \"encoder.encoder.layer.8.output.dense.weight\", \"encoder.encoder.layer.8.output.dense.bias\", \"encoder.encoder.layer.8.output.LayerNorm.weight\", \"encoder.encoder.layer.8.output.LayerNorm.bias\", \"encoder.encoder.layer.9.attention.self.query.weight\", \"encoder.encoder.layer.9.attention.self.query.bias\", \"encoder.encoder.layer.9.attention.self.key.weight\", \"encoder.encoder.layer.9.attention.self.key.bias\", \"encoder.encoder.layer.9.attention.self.value.weight\", \"encoder.encoder.layer.9.attention.self.value.bias\", \"encoder.encoder.layer.9.attention.output.dense.weight\", \"encoder.encoder.layer.9.attention.output.dense.bias\", \"encoder.encoder.layer.9.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.9.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.9.intermediate.dense.weight\", \"encoder.encoder.layer.9.intermediate.dense.bias\", \"encoder.encoder.layer.9.output.dense.weight\", \"encoder.encoder.layer.9.output.dense.bias\", \"encoder.encoder.layer.9.output.LayerNorm.weight\", \"encoder.encoder.layer.9.output.LayerNorm.bias\", \"encoder.encoder.layer.10.attention.self.query.weight\", \"encoder.encoder.layer.10.attention.self.query.bias\", \"encoder.encoder.layer.10.attention.self.key.weight\", \"encoder.encoder.layer.10.attention.self.key.bias\", \"encoder.encoder.layer.10.attention.self.value.weight\", \"encoder.encoder.layer.10.attention.self.value.bias\", \"encoder.encoder.layer.10.attention.output.dense.weight\", \"encoder.encoder.layer.10.attention.output.dense.bias\", \"encoder.encoder.layer.10.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.10.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.10.intermediate.dense.weight\", \"encoder.encoder.layer.10.intermediate.dense.bias\", \"encoder.encoder.layer.10.output.dense.weight\", \"encoder.encoder.layer.10.output.dense.bias\", \"encoder.encoder.layer.10.output.LayerNorm.weight\", \"encoder.encoder.layer.10.output.LayerNorm.bias\", \"encoder.encoder.layer.11.attention.self.query.weight\", \"encoder.encoder.layer.11.attention.self.query.bias\", \"encoder.encoder.layer.11.attention.self.key.weight\", \"encoder.encoder.layer.11.attention.self.key.bias\", \"encoder.encoder.layer.11.attention.self.value.weight\", \"encoder.encoder.layer.11.attention.self.value.bias\", \"encoder.encoder.layer.11.attention.output.dense.weight\", \"encoder.encoder.layer.11.attention.output.dense.bias\", \"encoder.encoder.layer.11.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.11.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.11.intermediate.dense.weight\", \"encoder.encoder.layer.11.intermediate.dense.bias\", \"encoder.encoder.layer.11.output.dense.weight\", \"encoder.encoder.layer.11.output.dense.bias\", \"encoder.encoder.layer.11.output.LayerNorm.weight\", \"encoder.encoder.layer.11.output.LayerNorm.bias\", \"encoder.pooler.dense.weight\", \"encoder.pooler.dense.bias\", \"sub_startlayer.weight\", \"sub_startlayer.bias\", \"sub_endlayer.weight\", \"sub_endlayer.bias\", \"obj_startlayer.weight\", \"obj_startlayer.bias\", \"obj_endlayer.weight\", \"obj_endlayer.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"embeddings.position_ids\", \"embeddings.word_embeddings.weight\", \"embeddings.position_embeddings.weight\", \"embeddings.token_type_embeddings.weight\", \"embeddings.LayerNorm.weight\", \"embeddings.LayerNorm.bias\", \"pooler.dense.weight\", \"pooler.dense.bias\", \"encoder.layer.0.attention.self.query.weight\", \"encoder.layer.0.attention.self.query.bias\", \"encoder.layer.0.attention.self.key.weight\", \"encoder.layer.0.attention.self.key.bias\", \"encoder.layer.0.attention.self.value.weight\", \"encoder.layer.0.attention.self.value.bias\", \"encoder.layer.0.attention.output.dense.weight\", \"encoder.layer.0.attention.output.dense.bias\", \"encoder.layer.0.attention.output.LayerNorm.weight\", \"encoder.layer.0.attention.output.LayerNorm.bias\", \"encoder.layer.0.intermediate.dense.weight\", \"encoder.layer.0.intermediate.dense.bias\", \"encoder.layer.0.output.dense.weight\", \"encoder.layer.0.output.dense.bias\", \"encoder.layer.0.output.LayerNorm.weight\", \"encoder.layer.0.output.LayerNorm.bias\", \"encoder.layer.1.attention.self.query.weight\", \"encoder.layer.1.attention.self.query.bias\", \"encoder.layer.1.attention.self.key.weight\", \"encoder.layer.1.attention.self.key.bias\", \"encoder.layer.1.attention.self.value.weight\", \"encoder.layer.1.attention.self.value.bias\", \"encoder.layer.1.attention.output.dense.weight\", \"encoder.layer.1.attention.output.dense.bias\", \"encoder.layer.1.attention.output.LayerNorm.weight\", \"encoder.layer.1.attention.output.LayerNorm.bias\", \"encoder.layer.1.intermediate.dense.weight\", \"encoder.layer.1.intermediate.dense.bias\", \"encoder.layer.1.output.dense.weight\", \"encoder.layer.1.output.dense.bias\", \"encoder.layer.1.output.LayerNorm.weight\", \"encoder.layer.1.output.LayerNorm.bias\", \"encoder.layer.2.attention.self.query.weight\", \"encoder.layer.2.attention.self.query.bias\", \"encoder.layer.2.attention.self.key.weight\", \"encoder.layer.2.attention.self.key.bias\", \"encoder.layer.2.attention.self.value.weight\", \"encoder.layer.2.attention.self.value.bias\", \"encoder.layer.2.attention.output.dense.weight\", \"encoder.layer.2.attention.output.dense.bias\", \"encoder.layer.2.attention.output.LayerNorm.weight\", \"encoder.layer.2.attention.output.LayerNorm.bias\", \"encoder.layer.2.intermediate.dense.weight\", \"encoder.layer.2.intermediate.dense.bias\", \"encoder.layer.2.output.dense.weight\", \"encoder.layer.2.output.dense.bias\", \"encoder.layer.2.output.LayerNorm.weight\", \"encoder.layer.2.output.LayerNorm.bias\", \"encoder.layer.3.attention.self.query.weight\", \"encoder.layer.3.attention.self.query.bias\", \"encoder.layer.3.attention.self.key.weight\", \"encoder.layer.3.attention.self.key.bias\", \"encoder.layer.3.attention.self.value.weight\", \"encoder.layer.3.attention.self.value.bias\", \"encoder.layer.3.attention.output.dense.weight\", \"encoder.layer.3.attention.output.dense.bias\", \"encoder.layer.3.attention.output.LayerNorm.weight\", \"encoder.layer.3.attention.output.LayerNorm.bias\", \"encoder.layer.3.intermediate.dense.weight\", \"encoder.layer.3.intermediate.dense.bias\", \"encoder.layer.3.output.dense.weight\", \"encoder.layer.3.output.dense.bias\", \"encoder.layer.3.output.LayerNorm.weight\", \"encoder.layer.3.output.LayerNorm.bias\", \"encoder.layer.4.attention.self.query.weight\", \"encoder.layer.4.attention.self.query.bias\", \"encoder.layer.4.attention.self.key.weight\", \"encoder.layer.4.attention.self.key.bias\", \"encoder.layer.4.attention.self.value.weight\", \"encoder.layer.4.attention.self.value.bias\", \"encoder.layer.4.attention.output.dense.weight\", \"encoder.layer.4.attention.output.dense.bias\", \"encoder.layer.4.attention.output.LayerNorm.weight\", \"encoder.layer.4.attention.output.LayerNorm.bias\", \"encoder.layer.4.intermediate.dense.weight\", \"encoder.layer.4.intermediate.dense.bias\", \"encoder.layer.4.output.dense.weight\", \"encoder.layer.4.output.dense.bias\", \"encoder.layer.4.output.LayerNorm.weight\", \"encoder.layer.4.output.LayerNorm.bias\", \"encoder.layer.5.attention.self.query.weight\", \"encoder.layer.5.attention.self.query.bias\", \"encoder.layer.5.attention.self.key.weight\", \"encoder.layer.5.attention.self.key.bias\", \"encoder.layer.5.attention.self.value.weight\", \"encoder.layer.5.attention.self.value.bias\", \"encoder.layer.5.attention.output.dense.weight\", \"encoder.layer.5.attention.output.dense.bias\", \"encoder.layer.5.attention.output.LayerNorm.weight\", \"encoder.layer.5.attention.output.LayerNorm.bias\", \"encoder.layer.5.intermediate.dense.weight\", \"encoder.layer.5.intermediate.dense.bias\", \"encoder.layer.5.output.dense.weight\", \"encoder.layer.5.output.dense.bias\", \"encoder.layer.5.output.LayerNorm.weight\", \"encoder.layer.5.output.LayerNorm.bias\", \"encoder.layer.6.attention.self.query.weight\", \"encoder.layer.6.attention.self.query.bias\", \"encoder.layer.6.attention.self.key.weight\", \"encoder.layer.6.attention.self.key.bias\", \"encoder.layer.6.attention.self.value.weight\", \"encoder.layer.6.attention.self.value.bias\", \"encoder.layer.6.attention.output.dense.weight\", \"encoder.layer.6.attention.output.dense.bias\", \"encoder.layer.6.attention.output.LayerNorm.weight\", \"encoder.layer.6.attention.output.LayerNorm.bias\", \"encoder.layer.6.intermediate.dense.weight\", \"encoder.layer.6.intermediate.dense.bias\", \"encoder.layer.6.output.dense.weight\", \"encoder.layer.6.output.dense.bias\", \"encoder.layer.6.output.LayerNorm.weight\", \"encoder.layer.6.output.LayerNorm.bias\", \"encoder.layer.7.attention.self.query.weight\", \"encoder.layer.7.attention.self.query.bias\", \"encoder.layer.7.attention.self.key.weight\", \"encoder.layer.7.attention.self.key.bias\", \"encoder.layer.7.attention.self.value.weight\", \"encoder.layer.7.attention.self.value.bias\", \"encoder.layer.7.attention.output.dense.weight\", \"encoder.layer.7.attention.output.dense.bias\", \"encoder.layer.7.attention.output.LayerNorm.weight\", \"encoder.layer.7.attention.output.LayerNorm.bias\", \"encoder.layer.7.intermediate.dense.weight\", \"encoder.layer.7.intermediate.dense.bias\", \"encoder.layer.7.output.dense.weight\", \"encoder.layer.7.output.dense.bias\", \"encoder.layer.7.output.LayerNorm.weight\", \"encoder.layer.7.output.LayerNorm.bias\", \"encoder.layer.8.attention.self.query.weight\", \"encoder.layer.8.attention.self.query.bias\", \"encoder.layer.8.attention.self.key.weight\", \"encoder.layer.8.attention.self.key.bias\", \"encoder.layer.8.attention.self.value.weight\", \"encoder.layer.8.attention.self.value.bias\", \"encoder.layer.8.attention.output.dense.weight\", \"encoder.layer.8.attention.output.dense.bias\", \"encoder.layer.8.attention.output.LayerNorm.weight\", \"encoder.layer.8.attention.output.LayerNorm.bias\", \"encoder.layer.8.intermediate.dense.weight\", \"encoder.layer.8.intermediate.dense.bias\", \"encoder.layer.8.output.dense.weight\", \"encoder.layer.8.output.dense.bias\", \"encoder.layer.8.output.LayerNorm.weight\", \"encoder.layer.8.output.LayerNorm.bias\", \"encoder.layer.9.attention.self.query.weight\", \"encoder.layer.9.attention.self.query.bias\", \"encoder.layer.9.attention.self.key.weight\", \"encoder.layer.9.attention.self.key.bias\", \"encoder.layer.9.attention.self.value.weight\", \"encoder.layer.9.attention.self.value.bias\", \"encoder.layer.9.attention.output.dense.weight\", \"encoder.layer.9.attention.output.dense.bias\", \"encoder.layer.9.attention.output.LayerNorm.weight\", \"encoder.layer.9.attention.output.LayerNorm.bias\", \"encoder.layer.9.intermediate.dense.weight\", \"encoder.layer.9.intermediate.dense.bias\", \"encoder.layer.9.output.dense.weight\", \"encoder.layer.9.output.dense.bias\", \"encoder.layer.9.output.LayerNorm.weight\", \"encoder.layer.9.output.LayerNorm.bias\", \"encoder.layer.10.attention.self.query.weight\", \"encoder.layer.10.attention.self.query.bias\", \"encoder.layer.10.attention.self.key.weight\", \"encoder.layer.10.attention.self.key.bias\", \"encoder.layer.10.attention.self.value.weight\", \"encoder.layer.10.attention.self.value.bias\", \"encoder.layer.10.attention.output.dense.weight\", \"encoder.layer.10.attention.output.dense.bias\", \"encoder.layer.10.attention.output.LayerNorm.weight\", \"encoder.layer.10.attention.output.LayerNorm.bias\", \"encoder.layer.10.intermediate.dense.weight\", \"encoder.layer.10.intermediate.dense.bias\", \"encoder.layer.10.output.dense.weight\", \"encoder.layer.10.output.dense.bias\", \"encoder.layer.10.output.LayerNorm.weight\", \"encoder.layer.10.output.LayerNorm.bias\", \"encoder.layer.11.attention.self.query.weight\", \"encoder.layer.11.attention.self.query.bias\", \"encoder.layer.11.attention.self.key.weight\", \"encoder.layer.11.attention.self.key.bias\", \"encoder.layer.11.attention.self.value.weight\", \"encoder.layer.11.attention.self.value.bias\", \"encoder.layer.11.attention.output.dense.weight\", \"encoder.layer.11.attention.output.dense.bias\", \"encoder.layer.11.attention.output.LayerNorm.weight\", \"encoder.layer.11.attention.output.LayerNorm.bias\", \"encoder.layer.11.intermediate.dense.weight\", \"encoder.layer.11.intermediate.dense.bias\", \"encoder.layer.11.output.dense.weight\", \"encoder.layer.11.output.dense.bias\", \"encoder.layer.11.output.LayerNorm.weight\", \"encoder.layer.11.output.LayerNorm.bias\". \n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=3  bash examples/run_ie.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e483d0e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running\n",
      "06/14/2022 11:26:57 - INFO - root -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, data_dir='CBLUEDatasets', device=device(type='cuda'), do_predict=True, do_train=False, earlystop_patience=2, epochs=3, eval_batch_size=32, learning_rate=5e-05, logging_steps=10, max_grad_norm=1.0, max_length=128, model_dir='data/model_data', model_name='chinese-bert-wwm-ext', model_type='bert', output_dir='data/output/ee/chinese-bert-wwm-ext', result_output_dir='data/result_output', save_steps=1000, seed=2021, task_name='ee', train_batch_size=8, warmup_proportion=0.1, weight_decay=0.01)\n",
      "06/14/2022 11:27:02 - INFO - root -   ***** Running prediction *****\n",
      "06/14/2022 11:27:02 - INFO - root -   Num samples 3000\n",
      "[Prediction] 94/94 [==============================] 203.4ms/step"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=3  bash examples/run_ie.sh predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66cc1f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
