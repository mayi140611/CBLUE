{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f581cfd9",
   "metadata": {},
   "source": [
    "# prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a9c68f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18aeba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cda34f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d01245d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5525f83fe949468b53887fe6c23beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/19.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a12348ed29346468d1b3248ba42c2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/647 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4d3d6fd0aa41fe90554d978c4e6082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/107k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47398f3137194d078341df94437795d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/263k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7310717663a94cab947b8b0c4cab2601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9031866c8338421d83d3a7335d02b215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"hfl/chinese-bert-wwm-ext\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c72e366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ea0f97b23a4552ad4028cd0ba817ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/393M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel \n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7534af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('data/model_data/chinese-bert-wwm-ext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1d9cc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/model_data/chinese-bert-wwm-ext/tokenizer_config.json',\n",
       " 'data/model_data/chinese-bert-wwm-ext/special_tokens_map.json',\n",
       " 'data/model_data/chinese-bert-wwm-ext/vocab.txt',\n",
       " 'data/model_data/chinese-bert-wwm-ext/added_tokens.json',\n",
       " 'data/model_data/chinese-bert-wwm-ext/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('data/model_data/chinese-bert-wwm-ext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e90cfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 14 20:18:05 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   69C    P0    67W /  70W |  14717MiB / 15109MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla T4            Off  | 00000000:00:07.0 Off |                    0 |\n",
      "| N/A   28C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla T4            Off  | 00000000:00:08.0 Off |                    0 |\n",
      "| N/A   69C    P0    69W /  70W |   8282MiB / 15109MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla T4            Off  | 00000000:00:09.0 Off |                    0 |\n",
      "| N/A   64C    P0    70W /  70W |   5998MiB / 15109MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac9f60",
   "metadata": {},
   "source": [
    "# CMeEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63a5e6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.0.10-py2.py3-none-any.whl (242 kB)\n",
      "\u001b[K     |████████████████████████████████| 242 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.0.10\n"
     ]
    }
   ],
   "source": [
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ee750",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running\n",
      "06/14/2022 20:20:06 - INFO - root -   Training CLS model...\n",
      "Building prefix dict from the default dictionary ...\n",
      "06/14/2022 20:20:10 - DEBUG - jieba -   Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "06/14/2022 20:20:10 - DEBUG - jieba -   Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.805 seconds.\n",
      "06/14/2022 20:20:11 - DEBUG - jieba -   Loading model cost 0.805 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "06/14/2022 20:20:11 - DEBUG - jieba -   Prefix dict has been built successfully.\n",
      "06/14/2022 20:20:12 - INFO - gensim.corpora.dictionary -   adding document #0 to Dictionary<0 unique tokens: []>\n",
      "06/14/2022 20:20:12 - INFO - gensim.corpora.dictionary -   adding document #10000 to Dictionary<4399 unique tokens: ['霍乱', ',', '01', '型', '所致']...>\n",
      "06/14/2022 20:20:13 - INFO - gensim.corpora.dictionary -   adding document #20000 to Dictionary<6737 unique tokens: ['霍乱', ',', '01', '型', '所致']...>\n",
      "06/14/2022 20:20:13 - INFO - gensim.corpora.dictionary -   adding document #30000 to Dictionary<8736 unique tokens: ['霍乱', ',', '01', '型', '所致']...>\n",
      "06/14/2022 20:20:13 - INFO - gensim.corpora.dictionary -   built Dictionary<10473 unique tokens: ['霍乱', ',', '01', '型', '所致']...> from 37879 documents (total 204725 corpus positions)\n",
      "06/14/2022 20:20:13 - INFO - gensim.utils -   Dictionary lifecycle event {'msg': \"built Dictionary<10473 unique tokens: ['霍乱', ',', '01', '型', '所致']...> from 37879 documents (total 204725 corpus positions)\", 'datetime': '2022-06-14T20:20:13.321081', 'gensim': '4.2.0', 'python': '3.7.12 (default, Jan 15 2022, 18:48:18) \\n[GCC 7.5.0]', 'platform': 'Linux-3.10.0-1160.42.2.el7.x86_64-x86_64-with-Ubuntu-18.04-bionic', 'event': 'created'}\n",
      "06/14/2022 20:20:13 - INFO - gensim.models.tfidfmodel -   collecting document frequencies\n",
      "06/14/2022 20:20:13 - INFO - gensim.models.tfidfmodel -   PROGRESS: processing document #0\n",
      "06/14/2022 20:20:13 - INFO - gensim.models.tfidfmodel -   PROGRESS: processing document #10000\n",
      "06/14/2022 20:20:13 - INFO - gensim.models.tfidfmodel -   PROGRESS: processing document #20000\n",
      "06/14/2022 20:20:13 - INFO - gensim.models.tfidfmodel -   PROGRESS: processing document #30000\n",
      "06/14/2022 20:20:13 - INFO - gensim.utils -   TfidfModel lifecycle event {'msg': 'calculated IDF weights for 37879 documents and 10473 features (196761 matrix non-zeros)', 'datetime': '2022-06-14T20:20:13.666481', 'gensim': '4.2.0', 'python': '3.7.12 (default, Jan 15 2022, 18:48:18) \\n[GCC 7.5.0]', 'platform': 'Linux-3.10.0-1160.42.2.el7.x86_64-x86_64-with-Ubuntu-18.04-bionic', 'event': 'initialize'}\n",
      "06/14/2022 20:20:13 - INFO - gensim.similarities.docsim -   creating sparse index\n",
      "06/14/2022 20:20:13 - INFO - gensim.matutils -   creating sparse matrix from corpus\n",
      "06/14/2022 20:20:13 - INFO - gensim.matutils -   PROGRESS: at document #0\n",
      "06/14/2022 20:20:13 - INFO - gensim.matutils -   PROGRESS: at document #10000\n",
      "06/14/2022 20:20:14 - INFO - gensim.matutils -   PROGRESS: at document #20000\n",
      "06/14/2022 20:20:14 - INFO - gensim.matutils -   PROGRESS: at document #30000\n",
      "06/14/2022 20:20:14 - INFO - gensim.similarities.docsim -   created <37879x10473 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 196761 stored elements in Compressed Sparse Row format>\n",
      " 41%|███████████████▉                       | 2448/6000 [02:48<03:56, 15.01it/s]"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1  bash examples/run_cdn.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "010b2cc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running\n",
      "06/14/2022 11:26:57 - INFO - root -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, data_dir='CBLUEDatasets', device=device(type='cuda'), do_predict=True, do_train=False, earlystop_patience=2, epochs=3, eval_batch_size=32, learning_rate=5e-05, logging_steps=10, max_grad_norm=1.0, max_length=128, model_dir='data/model_data', model_name='chinese-bert-wwm-ext', model_type='bert', output_dir='data/output/ee/chinese-bert-wwm-ext', result_output_dir='data/result_output', save_steps=1000, seed=2021, task_name='ee', train_batch_size=8, warmup_proportion=0.1, weight_decay=0.01)\n",
      "06/14/2022 11:27:02 - INFO - root -   ***** Running prediction *****\n",
      "06/14/2022 11:27:02 - INFO - root -   Num samples 3000\n",
      "[Prediction] 94/94 [==============================] 203.4ms/step"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=3  bash examples/run_cdn.sh predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6c33a",
   "metadata": {},
   "source": [
    "# CMeIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca3f78e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running\n",
      "06/14/2022 20:13:01 - INFO - root -   Training ER model...\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "06/14/2022 20:13:05 - INFO - root -   ***** Running training *****\n",
      "06/14/2022 20:13:05 - INFO - root -   Num samples 14339\n",
      "06/14/2022 20:13:05 - INFO - root -   Num epochs 1\n",
      "06/14/2022 20:13:05 - INFO - root -   Num training steps 449\n",
      "06/14/2022 20:13:05 - INFO - root -   Num warmup steps 44\n",
      "[Training] 200/449 [============>.................] - ETA: 2:44  loss: 0.0909 \n",
      "06/14/2022 20:15:18 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 20:15:18 - INFO - root -   Num samples 3585\n",
      "06/14/2022 20:16:05 - INFO - root -   ie-checkpoint-3000 f1 score: 0.8931395268041857\n",
      "06/14/2022 20:16:06 - INFO - root -   Saving models checkpoint to data/output/ie/checkpoint-3000/checkpoint-200\n",
      "[Training] 253/449 [===============>..............] - ETA: 2:48  loss: 0.0751 "
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=3  bash examples/run_ie.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb68160d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running\n",
      "06/14/2022 19:55:40 - INFO - root -   Training ER model...\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "06/14/2022 19:55:45 - INFO - root -   ***** Running training *****\n",
      "06/14/2022 19:55:45 - INFO - root -   Num samples 14339\n",
      "06/14/2022 19:55:45 - INFO - root -   Num epochs 1\n",
      "06/14/2022 19:55:45 - INFO - root -   Num training steps 449\n",
      "06/14/2022 19:55:45 - INFO - root -   Num warmup steps 44\n",
      "[Training] 200/449 [============>.................] - ETA: 2:45  loss: 0.2465 \n",
      "06/14/2022 19:57:58 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 19:57:58 - INFO - root -   Num samples 3585\n",
      "06/14/2022 19:58:45 - INFO - root -   ie-chinese-bert-wwm-ext f1 score: 0.8246199385390132\n",
      "06/14/2022 19:58:46 - INFO - root -   Saving models checkpoint to data/output/ie/chinese-bert-wwm-ext/checkpoint-200\n",
      "[Training] 400/449 [=========================>....] - ETA: 39s  loss: 0.1753  \n",
      "06/14/2022 20:01:06 - INFO - root -   ***** Running evaluation *****\n",
      "06/14/2022 20:01:06 - INFO - root -   Num samples 3585\n",
      "06/14/2022 20:01:54 - INFO - root -   ie-chinese-bert-wwm-ext f1 score: 0.8537684861784975\n",
      "06/14/2022 20:01:55 - INFO - root -   Saving models checkpoint to data/output/ie/chinese-bert-wwm-ext/checkpoint-400\n",
      "[Training] 449/449 [==============================] 898.2ms/step  loss: 0.1518 06/14/2022 20:02:28 - INFO - root -   Training Stop! The best step 400: 0.8537684861784975\n",
      "Traceback (most recent call last):\n",
      "  File \"baselines/run_ie.py\", line 220, in <module>\n",
      "    main()\n",
      "  File \"baselines/run_ie.py\", line 123, in main\n",
      "    model.load_state_dict(torch.load(os.path.join(args.output_dir, f'checkpoint-{best_step}', 'pytorch_model.bin')))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1498, in load_state_dict\n",
      "    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
      "RuntimeError: Error(s) in loading state_dict for ERModel:\n",
      "\tMissing key(s) in state_dict: \"encoder.embeddings.position_ids\", \"encoder.embeddings.word_embeddings.weight\", \"encoder.embeddings.position_embeddings.weight\", \"encoder.embeddings.token_type_embeddings.weight\", \"encoder.embeddings.LayerNorm.weight\", \"encoder.embeddings.LayerNorm.bias\", \"encoder.encoder.layer.0.attention.self.query.weight\", \"encoder.encoder.layer.0.attention.self.query.bias\", \"encoder.encoder.layer.0.attention.self.key.weight\", \"encoder.encoder.layer.0.attention.self.key.bias\", \"encoder.encoder.layer.0.attention.self.value.weight\", \"encoder.encoder.layer.0.attention.self.value.bias\", \"encoder.encoder.layer.0.attention.output.dense.weight\", \"encoder.encoder.layer.0.attention.output.dense.bias\", \"encoder.encoder.layer.0.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.0.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.0.intermediate.dense.weight\", \"encoder.encoder.layer.0.intermediate.dense.bias\", \"encoder.encoder.layer.0.output.dense.weight\", \"encoder.encoder.layer.0.output.dense.bias\", \"encoder.encoder.layer.0.output.LayerNorm.weight\", \"encoder.encoder.layer.0.output.LayerNorm.bias\", \"encoder.encoder.layer.1.attention.self.query.weight\", \"encoder.encoder.layer.1.attention.self.query.bias\", \"encoder.encoder.layer.1.attention.self.key.weight\", \"encoder.encoder.layer.1.attention.self.key.bias\", \"encoder.encoder.layer.1.attention.self.value.weight\", \"encoder.encoder.layer.1.attention.self.value.bias\", \"encoder.encoder.layer.1.attention.output.dense.weight\", \"encoder.encoder.layer.1.attention.output.dense.bias\", \"encoder.encoder.layer.1.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.1.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.1.intermediate.dense.weight\", \"encoder.encoder.layer.1.intermediate.dense.bias\", \"encoder.encoder.layer.1.output.dense.weight\", \"encoder.encoder.layer.1.output.dense.bias\", \"encoder.encoder.layer.1.output.LayerNorm.weight\", \"encoder.encoder.layer.1.output.LayerNorm.bias\", \"encoder.encoder.layer.2.attention.self.query.weight\", \"encoder.encoder.layer.2.attention.self.query.bias\", \"encoder.encoder.layer.2.attention.self.key.weight\", \"encoder.encoder.layer.2.attention.self.key.bias\", \"encoder.encoder.layer.2.attention.self.value.weight\", \"encoder.encoder.layer.2.attention.self.value.bias\", \"encoder.encoder.layer.2.attention.output.dense.weight\", \"encoder.encoder.layer.2.attention.output.dense.bias\", \"encoder.encoder.layer.2.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.2.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.2.intermediate.dense.weight\", \"encoder.encoder.layer.2.intermediate.dense.bias\", \"encoder.encoder.layer.2.output.dense.weight\", \"encoder.encoder.layer.2.output.dense.bias\", \"encoder.encoder.layer.2.output.LayerNorm.weight\", \"encoder.encoder.layer.2.output.LayerNorm.bias\", \"encoder.encoder.layer.3.attention.self.query.weight\", \"encoder.encoder.layer.3.attention.self.query.bias\", \"encoder.encoder.layer.3.attention.self.key.weight\", \"encoder.encoder.layer.3.attention.self.key.bias\", \"encoder.encoder.layer.3.attention.self.value.weight\", \"encoder.encoder.layer.3.attention.self.value.bias\", \"encoder.encoder.layer.3.attention.output.dense.weight\", \"encoder.encoder.layer.3.attention.output.dense.bias\", \"encoder.encoder.layer.3.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.3.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.3.intermediate.dense.weight\", \"encoder.encoder.layer.3.intermediate.dense.bias\", \"encoder.encoder.layer.3.output.dense.weight\", \"encoder.encoder.layer.3.output.dense.bias\", \"encoder.encoder.layer.3.output.LayerNorm.weight\", \"encoder.encoder.layer.3.output.LayerNorm.bias\", \"encoder.encoder.layer.4.attention.self.query.weight\", \"encoder.encoder.layer.4.attention.self.query.bias\", \"encoder.encoder.layer.4.attention.self.key.weight\", \"encoder.encoder.layer.4.attention.self.key.bias\", \"encoder.encoder.layer.4.attention.self.value.weight\", \"encoder.encoder.layer.4.attention.self.value.bias\", \"encoder.encoder.layer.4.attention.output.dense.weight\", \"encoder.encoder.layer.4.attention.output.dense.bias\", \"encoder.encoder.layer.4.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.4.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.4.intermediate.dense.weight\", \"encoder.encoder.layer.4.intermediate.dense.bias\", \"encoder.encoder.layer.4.output.dense.weight\", \"encoder.encoder.layer.4.output.dense.bias\", \"encoder.encoder.layer.4.output.LayerNorm.weight\", \"encoder.encoder.layer.4.output.LayerNorm.bias\", \"encoder.encoder.layer.5.attention.self.query.weight\", \"encoder.encoder.layer.5.attention.self.query.bias\", \"encoder.encoder.layer.5.attention.self.key.weight\", \"encoder.encoder.layer.5.attention.self.key.bias\", \"encoder.encoder.layer.5.attention.self.value.weight\", \"encoder.encoder.layer.5.attention.self.value.bias\", \"encoder.encoder.layer.5.attention.output.dense.weight\", \"encoder.encoder.layer.5.attention.output.dense.bias\", \"encoder.encoder.layer.5.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.5.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.5.intermediate.dense.weight\", \"encoder.encoder.layer.5.intermediate.dense.bias\", \"encoder.encoder.layer.5.output.dense.weight\", \"encoder.encoder.layer.5.output.dense.bias\", \"encoder.encoder.layer.5.output.LayerNorm.weight\", \"encoder.encoder.layer.5.output.LayerNorm.bias\", \"encoder.encoder.layer.6.attention.self.query.weight\", \"encoder.encoder.layer.6.attention.self.query.bias\", \"encoder.encoder.layer.6.attention.self.key.weight\", \"encoder.encoder.layer.6.attention.self.key.bias\", \"encoder.encoder.layer.6.attention.self.value.weight\", \"encoder.encoder.layer.6.attention.self.value.bias\", \"encoder.encoder.layer.6.attention.output.dense.weight\", \"encoder.encoder.layer.6.attention.output.dense.bias\", \"encoder.encoder.layer.6.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.6.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.6.intermediate.dense.weight\", \"encoder.encoder.layer.6.intermediate.dense.bias\", \"encoder.encoder.layer.6.output.dense.weight\", \"encoder.encoder.layer.6.output.dense.bias\", \"encoder.encoder.layer.6.output.LayerNorm.weight\", \"encoder.encoder.layer.6.output.LayerNorm.bias\", \"encoder.encoder.layer.7.attention.self.query.weight\", \"encoder.encoder.layer.7.attention.self.query.bias\", \"encoder.encoder.layer.7.attention.self.key.weight\", \"encoder.encoder.layer.7.attention.self.key.bias\", \"encoder.encoder.layer.7.attention.self.value.weight\", \"encoder.encoder.layer.7.attention.self.value.bias\", \"encoder.encoder.layer.7.attention.output.dense.weight\", \"encoder.encoder.layer.7.attention.output.dense.bias\", \"encoder.encoder.layer.7.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.7.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.7.intermediate.dense.weight\", \"encoder.encoder.layer.7.intermediate.dense.bias\", \"encoder.encoder.layer.7.output.dense.weight\", \"encoder.encoder.layer.7.output.dense.bias\", \"encoder.encoder.layer.7.output.LayerNorm.weight\", \"encoder.encoder.layer.7.output.LayerNorm.bias\", \"encoder.encoder.layer.8.attention.self.query.weight\", \"encoder.encoder.layer.8.attention.self.query.bias\", \"encoder.encoder.layer.8.attention.self.key.weight\", \"encoder.encoder.layer.8.attention.self.key.bias\", \"encoder.encoder.layer.8.attention.self.value.weight\", \"encoder.encoder.layer.8.attention.self.value.bias\", \"encoder.encoder.layer.8.attention.output.dense.weight\", \"encoder.encoder.layer.8.attention.output.dense.bias\", \"encoder.encoder.layer.8.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.8.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.8.intermediate.dense.weight\", \"encoder.encoder.layer.8.intermediate.dense.bias\", \"encoder.encoder.layer.8.output.dense.weight\", \"encoder.encoder.layer.8.output.dense.bias\", \"encoder.encoder.layer.8.output.LayerNorm.weight\", \"encoder.encoder.layer.8.output.LayerNorm.bias\", \"encoder.encoder.layer.9.attention.self.query.weight\", \"encoder.encoder.layer.9.attention.self.query.bias\", \"encoder.encoder.layer.9.attention.self.key.weight\", \"encoder.encoder.layer.9.attention.self.key.bias\", \"encoder.encoder.layer.9.attention.self.value.weight\", \"encoder.encoder.layer.9.attention.self.value.bias\", \"encoder.encoder.layer.9.attention.output.dense.weight\", \"encoder.encoder.layer.9.attention.output.dense.bias\", \"encoder.encoder.layer.9.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.9.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.9.intermediate.dense.weight\", \"encoder.encoder.layer.9.intermediate.dense.bias\", \"encoder.encoder.layer.9.output.dense.weight\", \"encoder.encoder.layer.9.output.dense.bias\", \"encoder.encoder.layer.9.output.LayerNorm.weight\", \"encoder.encoder.layer.9.output.LayerNorm.bias\", \"encoder.encoder.layer.10.attention.self.query.weight\", \"encoder.encoder.layer.10.attention.self.query.bias\", \"encoder.encoder.layer.10.attention.self.key.weight\", \"encoder.encoder.layer.10.attention.self.key.bias\", \"encoder.encoder.layer.10.attention.self.value.weight\", \"encoder.encoder.layer.10.attention.self.value.bias\", \"encoder.encoder.layer.10.attention.output.dense.weight\", \"encoder.encoder.layer.10.attention.output.dense.bias\", \"encoder.encoder.layer.10.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.10.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.10.intermediate.dense.weight\", \"encoder.encoder.layer.10.intermediate.dense.bias\", \"encoder.encoder.layer.10.output.dense.weight\", \"encoder.encoder.layer.10.output.dense.bias\", \"encoder.encoder.layer.10.output.LayerNorm.weight\", \"encoder.encoder.layer.10.output.LayerNorm.bias\", \"encoder.encoder.layer.11.attention.self.query.weight\", \"encoder.encoder.layer.11.attention.self.query.bias\", \"encoder.encoder.layer.11.attention.self.key.weight\", \"encoder.encoder.layer.11.attention.self.key.bias\", \"encoder.encoder.layer.11.attention.self.value.weight\", \"encoder.encoder.layer.11.attention.self.value.bias\", \"encoder.encoder.layer.11.attention.output.dense.weight\", \"encoder.encoder.layer.11.attention.output.dense.bias\", \"encoder.encoder.layer.11.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.11.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.11.intermediate.dense.weight\", \"encoder.encoder.layer.11.intermediate.dense.bias\", \"encoder.encoder.layer.11.output.dense.weight\", \"encoder.encoder.layer.11.output.dense.bias\", \"encoder.encoder.layer.11.output.LayerNorm.weight\", \"encoder.encoder.layer.11.output.LayerNorm.bias\", \"encoder.pooler.dense.weight\", \"encoder.pooler.dense.bias\", \"sub_startlayer.weight\", \"sub_startlayer.bias\", \"sub_endlayer.weight\", \"sub_endlayer.bias\", \"obj_startlayer.weight\", \"obj_startlayer.bias\", \"obj_endlayer.weight\", \"obj_endlayer.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"embeddings.position_ids\", \"embeddings.word_embeddings.weight\", \"embeddings.position_embeddings.weight\", \"embeddings.token_type_embeddings.weight\", \"embeddings.LayerNorm.weight\", \"embeddings.LayerNorm.bias\", \"pooler.dense.weight\", \"pooler.dense.bias\", \"encoder.layer.0.attention.self.query.weight\", \"encoder.layer.0.attention.self.query.bias\", \"encoder.layer.0.attention.self.key.weight\", \"encoder.layer.0.attention.self.key.bias\", \"encoder.layer.0.attention.self.value.weight\", \"encoder.layer.0.attention.self.value.bias\", \"encoder.layer.0.attention.output.dense.weight\", \"encoder.layer.0.attention.output.dense.bias\", \"encoder.layer.0.attention.output.LayerNorm.weight\", \"encoder.layer.0.attention.output.LayerNorm.bias\", \"encoder.layer.0.intermediate.dense.weight\", \"encoder.layer.0.intermediate.dense.bias\", \"encoder.layer.0.output.dense.weight\", \"encoder.layer.0.output.dense.bias\", \"encoder.layer.0.output.LayerNorm.weight\", \"encoder.layer.0.output.LayerNorm.bias\", \"encoder.layer.1.attention.self.query.weight\", \"encoder.layer.1.attention.self.query.bias\", \"encoder.layer.1.attention.self.key.weight\", \"encoder.layer.1.attention.self.key.bias\", \"encoder.layer.1.attention.self.value.weight\", \"encoder.layer.1.attention.self.value.bias\", \"encoder.layer.1.attention.output.dense.weight\", \"encoder.layer.1.attention.output.dense.bias\", \"encoder.layer.1.attention.output.LayerNorm.weight\", \"encoder.layer.1.attention.output.LayerNorm.bias\", \"encoder.layer.1.intermediate.dense.weight\", \"encoder.layer.1.intermediate.dense.bias\", \"encoder.layer.1.output.dense.weight\", \"encoder.layer.1.output.dense.bias\", \"encoder.layer.1.output.LayerNorm.weight\", \"encoder.layer.1.output.LayerNorm.bias\", \"encoder.layer.2.attention.self.query.weight\", \"encoder.layer.2.attention.self.query.bias\", \"encoder.layer.2.attention.self.key.weight\", \"encoder.layer.2.attention.self.key.bias\", \"encoder.layer.2.attention.self.value.weight\", \"encoder.layer.2.attention.self.value.bias\", \"encoder.layer.2.attention.output.dense.weight\", \"encoder.layer.2.attention.output.dense.bias\", \"encoder.layer.2.attention.output.LayerNorm.weight\", \"encoder.layer.2.attention.output.LayerNorm.bias\", \"encoder.layer.2.intermediate.dense.weight\", \"encoder.layer.2.intermediate.dense.bias\", \"encoder.layer.2.output.dense.weight\", \"encoder.layer.2.output.dense.bias\", \"encoder.layer.2.output.LayerNorm.weight\", \"encoder.layer.2.output.LayerNorm.bias\", \"encoder.layer.3.attention.self.query.weight\", \"encoder.layer.3.attention.self.query.bias\", \"encoder.layer.3.attention.self.key.weight\", \"encoder.layer.3.attention.self.key.bias\", \"encoder.layer.3.attention.self.value.weight\", \"encoder.layer.3.attention.self.value.bias\", \"encoder.layer.3.attention.output.dense.weight\", \"encoder.layer.3.attention.output.dense.bias\", \"encoder.layer.3.attention.output.LayerNorm.weight\", \"encoder.layer.3.attention.output.LayerNorm.bias\", \"encoder.layer.3.intermediate.dense.weight\", \"encoder.layer.3.intermediate.dense.bias\", \"encoder.layer.3.output.dense.weight\", \"encoder.layer.3.output.dense.bias\", \"encoder.layer.3.output.LayerNorm.weight\", \"encoder.layer.3.output.LayerNorm.bias\", \"encoder.layer.4.attention.self.query.weight\", \"encoder.layer.4.attention.self.query.bias\", \"encoder.layer.4.attention.self.key.weight\", \"encoder.layer.4.attention.self.key.bias\", \"encoder.layer.4.attention.self.value.weight\", \"encoder.layer.4.attention.self.value.bias\", \"encoder.layer.4.attention.output.dense.weight\", \"encoder.layer.4.attention.output.dense.bias\", \"encoder.layer.4.attention.output.LayerNorm.weight\", \"encoder.layer.4.attention.output.LayerNorm.bias\", \"encoder.layer.4.intermediate.dense.weight\", \"encoder.layer.4.intermediate.dense.bias\", \"encoder.layer.4.output.dense.weight\", \"encoder.layer.4.output.dense.bias\", \"encoder.layer.4.output.LayerNorm.weight\", \"encoder.layer.4.output.LayerNorm.bias\", \"encoder.layer.5.attention.self.query.weight\", \"encoder.layer.5.attention.self.query.bias\", \"encoder.layer.5.attention.self.key.weight\", \"encoder.layer.5.attention.self.key.bias\", \"encoder.layer.5.attention.self.value.weight\", \"encoder.layer.5.attention.self.value.bias\", \"encoder.layer.5.attention.output.dense.weight\", \"encoder.layer.5.attention.output.dense.bias\", \"encoder.layer.5.attention.output.LayerNorm.weight\", \"encoder.layer.5.attention.output.LayerNorm.bias\", \"encoder.layer.5.intermediate.dense.weight\", \"encoder.layer.5.intermediate.dense.bias\", \"encoder.layer.5.output.dense.weight\", \"encoder.layer.5.output.dense.bias\", \"encoder.layer.5.output.LayerNorm.weight\", \"encoder.layer.5.output.LayerNorm.bias\", \"encoder.layer.6.attention.self.query.weight\", \"encoder.layer.6.attention.self.query.bias\", \"encoder.layer.6.attention.self.key.weight\", \"encoder.layer.6.attention.self.key.bias\", \"encoder.layer.6.attention.self.value.weight\", \"encoder.layer.6.attention.self.value.bias\", \"encoder.layer.6.attention.output.dense.weight\", \"encoder.layer.6.attention.output.dense.bias\", \"encoder.layer.6.attention.output.LayerNorm.weight\", \"encoder.layer.6.attention.output.LayerNorm.bias\", \"encoder.layer.6.intermediate.dense.weight\", \"encoder.layer.6.intermediate.dense.bias\", \"encoder.layer.6.output.dense.weight\", \"encoder.layer.6.output.dense.bias\", \"encoder.layer.6.output.LayerNorm.weight\", \"encoder.layer.6.output.LayerNorm.bias\", \"encoder.layer.7.attention.self.query.weight\", \"encoder.layer.7.attention.self.query.bias\", \"encoder.layer.7.attention.self.key.weight\", \"encoder.layer.7.attention.self.key.bias\", \"encoder.layer.7.attention.self.value.weight\", \"encoder.layer.7.attention.self.value.bias\", \"encoder.layer.7.attention.output.dense.weight\", \"encoder.layer.7.attention.output.dense.bias\", \"encoder.layer.7.attention.output.LayerNorm.weight\", \"encoder.layer.7.attention.output.LayerNorm.bias\", \"encoder.layer.7.intermediate.dense.weight\", \"encoder.layer.7.intermediate.dense.bias\", \"encoder.layer.7.output.dense.weight\", \"encoder.layer.7.output.dense.bias\", \"encoder.layer.7.output.LayerNorm.weight\", \"encoder.layer.7.output.LayerNorm.bias\", \"encoder.layer.8.attention.self.query.weight\", \"encoder.layer.8.attention.self.query.bias\", \"encoder.layer.8.attention.self.key.weight\", \"encoder.layer.8.attention.self.key.bias\", \"encoder.layer.8.attention.self.value.weight\", \"encoder.layer.8.attention.self.value.bias\", \"encoder.layer.8.attention.output.dense.weight\", \"encoder.layer.8.attention.output.dense.bias\", \"encoder.layer.8.attention.output.LayerNorm.weight\", \"encoder.layer.8.attention.output.LayerNorm.bias\", \"encoder.layer.8.intermediate.dense.weight\", \"encoder.layer.8.intermediate.dense.bias\", \"encoder.layer.8.output.dense.weight\", \"encoder.layer.8.output.dense.bias\", \"encoder.layer.8.output.LayerNorm.weight\", \"encoder.layer.8.output.LayerNorm.bias\", \"encoder.layer.9.attention.self.query.weight\", \"encoder.layer.9.attention.self.query.bias\", \"encoder.layer.9.attention.self.key.weight\", \"encoder.layer.9.attention.self.key.bias\", \"encoder.layer.9.attention.self.value.weight\", \"encoder.layer.9.attention.self.value.bias\", \"encoder.layer.9.attention.output.dense.weight\", \"encoder.layer.9.attention.output.dense.bias\", \"encoder.layer.9.attention.output.LayerNorm.weight\", \"encoder.layer.9.attention.output.LayerNorm.bias\", \"encoder.layer.9.intermediate.dense.weight\", \"encoder.layer.9.intermediate.dense.bias\", \"encoder.layer.9.output.dense.weight\", \"encoder.layer.9.output.dense.bias\", \"encoder.layer.9.output.LayerNorm.weight\", \"encoder.layer.9.output.LayerNorm.bias\", \"encoder.layer.10.attention.self.query.weight\", \"encoder.layer.10.attention.self.query.bias\", \"encoder.layer.10.attention.self.key.weight\", \"encoder.layer.10.attention.self.key.bias\", \"encoder.layer.10.attention.self.value.weight\", \"encoder.layer.10.attention.self.value.bias\", \"encoder.layer.10.attention.output.dense.weight\", \"encoder.layer.10.attention.output.dense.bias\", \"encoder.layer.10.attention.output.LayerNorm.weight\", \"encoder.layer.10.attention.output.LayerNorm.bias\", \"encoder.layer.10.intermediate.dense.weight\", \"encoder.layer.10.intermediate.dense.bias\", \"encoder.layer.10.output.dense.weight\", \"encoder.layer.10.output.dense.bias\", \"encoder.layer.10.output.LayerNorm.weight\", \"encoder.layer.10.output.LayerNorm.bias\", \"encoder.layer.11.attention.self.query.weight\", \"encoder.layer.11.attention.self.query.bias\", \"encoder.layer.11.attention.self.key.weight\", \"encoder.layer.11.attention.self.key.bias\", \"encoder.layer.11.attention.self.value.weight\", \"encoder.layer.11.attention.self.value.bias\", \"encoder.layer.11.attention.output.dense.weight\", \"encoder.layer.11.attention.output.dense.bias\", \"encoder.layer.11.attention.output.LayerNorm.weight\", \"encoder.layer.11.attention.output.LayerNorm.bias\", \"encoder.layer.11.intermediate.dense.weight\", \"encoder.layer.11.intermediate.dense.bias\", \"encoder.layer.11.output.dense.weight\", \"encoder.layer.11.output.dense.bias\", \"encoder.layer.11.output.LayerNorm.weight\", \"encoder.layer.11.output.LayerNorm.bias\". \n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=3  bash examples/run_ie.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e483d0e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running\n",
      "06/14/2022 11:26:57 - INFO - root -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, data_dir='CBLUEDatasets', device=device(type='cuda'), do_predict=True, do_train=False, earlystop_patience=2, epochs=3, eval_batch_size=32, learning_rate=5e-05, logging_steps=10, max_grad_norm=1.0, max_length=128, model_dir='data/model_data', model_name='chinese-bert-wwm-ext', model_type='bert', output_dir='data/output/ee/chinese-bert-wwm-ext', result_output_dir='data/result_output', save_steps=1000, seed=2021, task_name='ee', train_batch_size=8, warmup_proportion=0.1, weight_decay=0.01)\n",
      "06/14/2022 11:27:02 - INFO - root -   ***** Running prediction *****\n",
      "06/14/2022 11:27:02 - INFO - root -   Num samples 3000\n",
      "[Prediction] 94/94 [==============================] 203.4ms/step"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=3  bash examples/run_ie.sh predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66cc1f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
